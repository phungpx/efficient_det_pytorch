{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71143aca-5d90-44dc-bc5b-15390ca5a0b1",
   "metadata": {},
   "source": [
    "## 0 Efficient Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d637dd-85a7-48dc-8eed-4d306c2a6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EfficientNet.model import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c10bc6-676d-4cfa-be5b-0132a0db0bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "compound_coef = 0\n",
    "model = EfficientNet.from_pretrained(model_name=f'efficientnet-b{compound_coef}',\n",
    "                                     weights_path='../../../../checkpoint/efficientnet_pretrained_weight/efficientnet-b0-355c32eb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc937b9-a8d2-4827-8893-e6f8a7b449f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(size=224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225]),])\n",
    "\n",
    "def preprocess(image_path: str, device: str = 'cpu') -> torch.Tensor:\n",
    "    image = Image.open(image_path, mode='r').convert('RGB')\n",
    "    sample = transform(image)\n",
    "    print(sample.shape)\n",
    "    sample = sample.unsqueeze(dim=0)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e07f147-ad3a-42c4-8912-e95e502199a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 398])\n"
     ]
    }
   ],
   "source": [
    "sample = preprocess(image_path='/home/phungpx/Downloads/bird.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae06fb52-ae2e-45b7-a179-cc8a39e6af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: class_id=10, score=12.35792338848114\n",
      "Prediction: class_id=95, score=9.344076365232468\n",
      "Prediction: class_id=11, score=6.44075945019722\n",
      "Prediction: class_id=15, score=3.7864774465560913\n",
      "Prediction: class_id=16, score=2.5017768144607544\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model.eval().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(sample)\n",
    "\n",
    "for class_id in torch.topk(preds, k=5).indices.squeeze(0).tolist():\n",
    "    score = torch.softmax(preds, dim=1)[0, class_id].item()\n",
    "    print(f'Prediction: class_id={class_id}, score={100 * score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bb307-2f9e-41b3-94e1-dffb144f8c42",
   "metadata": {},
   "source": [
    "## 1.BackBone - EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79e59ea0-0096-480b-9e94-3e7839f5f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Number of Parameters: 3595388\n"
     ]
    }
   ],
   "source": [
    "from EfficientDet.EfficientNet.back_bone import EfficientNetBackBone\n",
    "feature_extractor = EfficientNetBackBone(compound_coef=0,\n",
    "                                         R_input=[512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536],\n",
    "                                         weight_path='../../../checkpoint/efficientnet_pretrained_weight/efficientnet-b0-355c32eb.pth')\n",
    "\n",
    "print(f'Number of Parameters: {sum(param.numel() for param in feature_extractor.parameters() if param.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f744b4e-07b4-49ae-a999-5a032a66c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.randn(1, 3, 512, 512)\n",
    "feature_maps = feature_extractor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b0f613f-3ca1-48ab-87e9-84cdc25a3492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P#1: torch.Size([1, 16, 256, 256])\n",
      "P#2: torch.Size([1, 24, 128, 128])\n",
      "P#3: torch.Size([1, 40, 64, 64])\n",
      "P#4: torch.Size([1, 112, 32, 32])\n",
      "P#5: torch.Size([1, 320, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for i, feature_map in enumerate(feature_maps, 1):\n",
    "    print(f'P#{i}: {feature_map.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098bb3f-2920-452d-b1cc-648af0fe8e50",
   "metadata": {},
   "source": [
    "## 2. BiFPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f18e4a6-5c1f-4c80-8e05-48e57eab2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EfficientDet.BiFPN.bifpn import BiFPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572b70b8-9169-4a80-865d-f5e02367e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 168249\n"
     ]
    }
   ],
   "source": [
    "backbone_out_channels = {0: [40, 112, 320], 1: [40, 112, 320],\n",
    "                         2: [48, 120, 352], 3: [48, 136, 384],\n",
    "                         4: [56, 160, 448], 5: [64, 176, 512],\n",
    "                         6: [72, 200, 576], 7: [72, 200, 576],\n",
    "                         8: [80, 224, 640]}\n",
    "\n",
    "bifpn = BiFPN(compound_coef=0,\n",
    "              backbone_out_channels=backbone_out_channels,\n",
    "              W_bifpn=[64, 88, 112, 160, 224, 288, 384, 384, 384],\n",
    "              D_bifpn=[3, 4, 5, 6, 7, 7, 8, 8, 8], onnx_export=False, epsilon=1e-4)\n",
    "\n",
    "print(f'Number of Parameters: {sum(param.numel() for param in bifpn.parameters() if param.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee4162d6-51da-4751-9644-b3f2d1815d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "P3, P4, P5 = feature_maps[-3:]\n",
    "pyramid_features = bifpn(feature_maps=(P3, P4, P5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a838e65-f1c2-48e0-aa02-6205808fa63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P#3: torch.Size([1, 64, 64, 64])\n",
      "P#4: torch.Size([1, 64, 32, 32])\n",
      "P#5: torch.Size([1, 64, 16, 16])\n",
      "P#6: torch.Size([1, 64, 8, 8])\n",
      "P#7: torch.Size([1, 64, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for i, pyramid_feature in enumerate(pyramid_features, 3):\n",
    "    print(f'P#{i}: {pyramid_feature.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff3d80-a66a-4eaf-918d-d2a08680531b",
   "metadata": {},
   "source": [
    "## 3. Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd966212-6f3c-47db-96e7-dd67ebc7fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EfficientDet.Head.regressor import Regressor\n",
    "from EfficientDet.Head.classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fcf380a-ffeb-42c5-a614-2997ad7cbcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters of Classifier: 22554\n",
      "Number of Parameters of Regressor: 19044\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(n_anchors=9,\n",
    "                        n_classes=10,\n",
    "                        compound_coef=0,\n",
    "                        D_class=[3, 3, 3, 4, 4, 4, 5, 5, 5],\n",
    "                        W_pred=[64, 88, 112, 160, 224, 288, 384, 384, 384],\n",
    "                        onnx_export=False)\n",
    "\n",
    "regressor = Regressor(n_anchors=9,\n",
    "                      compound_coef=0,\n",
    "                      D_box=[3, 3, 3, 4, 4, 4, 5, 5, 5],\n",
    "                      W_pred=[64, 88, 112, 160, 224, 288, 384, 384, 384],\n",
    "                      onnx_export=False)\n",
    "\n",
    "print(f'Number of Parameters of Classifier: {sum(param.numel() for param in classifier.parameters() if param.requires_grad)}')\n",
    "print(f'Number of Parameters of Regressor: {sum(param.numel() for param in regressor.parameters() if param.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3611731-d83f-4cbf-9189-4816bb2ee073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification: torch.Size([1, 49104, 10])\n",
      "regression: torch.Size([1, 49104, 4])\n"
     ]
    }
   ],
   "source": [
    "cls_preds = classifier(pyramid_features)\n",
    "loc_preds = regressor(pyramid_features)\n",
    "\n",
    "print(f'classification: {cls_preds.shape}')\n",
    "print(f'regression: {loc_preds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07f0ead9-c3f7-4b64-a9a5-785b300b2eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49104.0\n"
     ]
    }
   ],
   "source": [
    "# calculation out_channels of cls_preds, and loc_preds\n",
    "import numpy as np\n",
    "\n",
    "compound_coef = 0\n",
    "num_anchors = 9\n",
    "\n",
    "input_size = 512 + 128 * compound_coef\n",
    "\n",
    "pyramid_levels = np.arange(3, 8) if compound_coef <= 7 else np.arange(3, 9)\n",
    "\n",
    "out_channels = sum((input_size / 2 ** pyramid_levels) * (input_size / 2 ** pyramid_levels) * num_anchors)\n",
    "\n",
    "print(out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d38491-a73b-411a-b63e-18c95a7cae60",
   "metadata": {},
   "source": [
    "## 5. Efficient Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "124707ba-96aa-4816-9f37-65227422fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EfficientDet.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0103458e-1633-4d6e-b226-1890df050ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = Model(num_classes=1, compound_coef=0, scales=[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)],\n",
    "              aspect_ratios=[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "172af2fb-af4f-4931-9258-acc8d51ca320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 3799970\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Parameters: {sum(param.numel() for param in model.parameters() if param.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81c7f8bd-0138-4923-93f4-0b87d4d6cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49104, 1])\n",
      "torch.Size([1, 49104, 4])\n",
      "torch.Size([1, 49104, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cls_preds, loc_preds, anchors = model(inputs=torch.rand(1, 3, 512, 512))\n",
    "print(cls_preds.shape)\n",
    "print(loc_preds.shape)\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b146315-714d-49bf-b17c-15012ad84bf6",
   "metadata": {},
   "source": [
    "## Anchor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9db30d-94a4-4989-bb17-4115b7ebbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class AnchorGeneration(nn.Module):\n",
    "    def __init__(self,\n",
    "                 debug: bool = False,\n",
    "                 compound_coef: int = 0,\n",
    "                 scales: List[float] = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)],\n",
    "                 aspect_ratios: List[float] = [0.5, 1., 2.]) -> None:\n",
    "        super(AnchorGeneration, self).__init__()\n",
    "        self.anchor_scale = 4. if compound_coef != 7 else 5.\n",
    "\n",
    "        self.debug = debug\n",
    "        self.scales = scales\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, pyramid_features: Tuple[torch.Tensor]) -> torch.Tensor:\n",
    "        image_size = inputs.shape[-2:]\n",
    "        grid_sizes = [pyramid_feature.shape[-2:] for pyramid_feature in pyramid_features]\n",
    "        dtype, device = pyramid_features[0].dtype, pyramid_features[0].device\n",
    "        strides = [[image_size[0] // grid_size[0], image_size[1] // grid_size[1]] for grid_size in grid_sizes]\n",
    "\n",
    "        if self.debug:\n",
    "            visual_image = np.zeros(shape=(image_size[0], image_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "        anchors_over_all_pyramid_features = []\n",
    "        for stride in strides:\n",
    "            stride_height, stride_width = stride\n",
    "\n",
    "            anchors_per_pyramid_feature = []\n",
    "            for scale, aspect_ratio in itertools.product(self.scales, self.aspect_ratios):\n",
    "                if (image_size[0] % stride_height != 0) or (image_size[1] % stride_width != 0):\n",
    "                    raise ValueError('input size must be divided by the stride.')\n",
    "\n",
    "                base_anchor_width = self.anchor_scale * stride_width * scale\n",
    "                base_anchor_height = self.anchor_scale * stride_height * scale\n",
    "\n",
    "                anchor_width = base_anchor_width * np.sqrt(aspect_ratio)\n",
    "                anchor_height = base_anchor_height * (1 / np.sqrt(aspect_ratio))\n",
    "\n",
    "                shift_x = torch.arange(\n",
    "                    start=stride_width / 2, end=image_size[1], step=stride_width,\n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "                shift_y = torch.arange(\n",
    "                    start=stride_height / 2, end=image_size[0], step=stride_height,\n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "\n",
    "                shift_x, shift_y = torch.meshgrid(shift_x, shift_y)\n",
    "                shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)\n",
    "\n",
    "                # y1, x1, y2, x2\n",
    "                anchors = torch.stack(\n",
    "                    (shift_y - anchor_height / 2.,\n",
    "                     shift_x - anchor_width / 2.,\n",
    "                     shift_y + anchor_height / 2.,\n",
    "                     shift_x + anchor_width / 2.),\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                anchors_per_pyramid_feature.append(anchors)\n",
    "\n",
    "                if self.debug:\n",
    "                    import cv2\n",
    "                    for anchor in anchors:\n",
    "                        y1, x1, y2, x2 = anchor.numpy()\n",
    "                        cv2.rectangle(\n",
    "                            img=visual_image,\n",
    "                            pt1=(int(round(x1)), int(round(y1))),\n",
    "                            pt2=(int(round(x2)), int(round(y2))),\n",
    "                            color=(255, 255, 255),\n",
    "                            thickness=1\n",
    "                        )\n",
    "                    cv2.imshow(f'visual_at_stride_#{stride}', visual_image)\n",
    "                    cv2.waitKey()\n",
    "                    cv2.destroyAllWindows()\n",
    "\n",
    "            anchors_per_pyramid_feature = torch.cat(anchors_per_pyramid_feature, dim=0)\n",
    "\n",
    "            anchors_over_all_pyramid_features.append(anchors_per_pyramid_feature)\n",
    "\n",
    "        anchor_boxes = torch.cat(anchors_over_all_pyramid_features, dim=0).to(dtype).to(device)\n",
    "\n",
    "        return anchor_boxes.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df1dd36-2193-4bdc-b5f4-54b0c12683f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_generator = AnchorGeneration(debug=True,\n",
    "                                    compound_coef=0,\n",
    "                                    scales=[1 / 16, 1 / 8, 1 / 4],\n",
    "                                    aspect_ratios=[1 / 3, 0.5, 1., 2., 3.])\n",
    "\n",
    "inputs = torch.rand(1, 3, 512, 512)\n",
    "pyramid_features = [torch.rand(1, 3, 4, 4), torch.rand(1, 3, 8, 8)]\n",
    "\n",
    "anchor_boxes = anchor_generator(inputs, pyramid_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
