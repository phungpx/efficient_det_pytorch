{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be669f2b-4739-44a8-868d-d23199ed3e18",
   "metadata": {},
   "source": [
    "## 1. DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e16cd81-4084-410c-817b-fe8c73b5b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train:\n",
      "\t VOC2007: 5011\n",
      "\t VOC2012: 1464\n",
      "\t Total: 6475\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "from torch.utils.data import DataLoader\n",
    "from flame.core.data.pascal_dataset import PascalDataset\n",
    "\n",
    "classes2idx = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "               'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "               'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "               'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "\n",
    "transforms = [iaa.Add(value=(-10, 10), per_channel=True),\n",
    "              iaa.GaussianBlur(sigma=(0, 1)),\n",
    "              iaa.MotionBlur(),\n",
    "              iaa.JpegCompression(compression=(0, 10)),\n",
    "              iaa.Fliplr(p=0.5),\n",
    "              iaa.Flipud(p=0.5),\n",
    "              iaa.Grayscale(alpha=(0.0, 0.1)),\n",
    "              iaa.Affine(rotate=(-5, 5), shear=(-5, 5), fit_output=True),\n",
    "              iaa.Crop(percent=(0, 0.1)),\n",
    "              iaa.Pad(percent=(0, 0.1), keep_size=False),\n",
    "              iaa.ChangeColorTemperature()]\n",
    "\n",
    "pascal_dataset = PascalDataset(\n",
    "    VOC2012={\n",
    "        'image_dir': './dataset/PASCALVOC2012/JPEGImages/',\n",
    "        'label_dir': './dataset/PASCALVOC2012/Annotations/',\n",
    "        'txt_path': './dataset/PASCALVOC2012/ImageSets/Segmentation/train.txt'\n",
    "    },\n",
    "    VOC2007={\n",
    "        'image_dir': './dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/',\n",
    "        'label_dir': 'dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/'\n",
    "    },\n",
    "    image_extent='.jpg',\n",
    "    label_extent='.xml',\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "    compound_coef=0,\n",
    "    classes=classes2idx,\n",
    "    transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d59d25-85b9-4f99-83c1-fe786308ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_loader = DataLoader(\n",
    "    pascal_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float, device=device).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float, device=device).view(3, 1, 1)\n",
    "idx2class = {idx: label_name for label_name, idx in classes2idx.items()}\n",
    "\n",
    "for i, pascal_data in enumerate(iter(pascal_loader)):\n",
    "    samples, targets, sample_infos = pascal_data\n",
    "    for sample, target, sample_info in zip(samples, targets, sample_infos):\n",
    "        image = ((sample * std + mean) * 255).permute(1, 2, 0).contiguous()\n",
    "        image = image.to(torch.uint8).cpu().numpy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        boxes = target['boxes'].data.cpu().numpy().astype(np.int32)\n",
    "        labels = target['labels'].data.cpu().numpy().astype(np.int32)\n",
    "\n",
    "        thickness = max(sample_info[1]) // 800\n",
    "        fontscale = max(sample_info[1]) / 800\n",
    "\n",
    "        for box, label in zip(boxes, labels):\n",
    "            if label != -1:\n",
    "                image = np.ascontiguousarray(image)\n",
    "                cv2.rectangle(\n",
    "                    img=image,\n",
    "                    pt1=tuple(box[:2]),\n",
    "                    pt2=tuple(box[2:]),    \n",
    "                    color=(0, 255, 0),\n",
    "                    thickness=thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    img=image,\n",
    "                    text=idx2class[label.item()],\n",
    "                    org=tuple(box[:2]),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=fontscale,\n",
    "                    color=(0, 0, 255),\n",
    "                    thickness=thickness,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(sample_info[0], image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cdf72-6115-4a4b-b5cf-a09ada3581cb",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3808523-ae0c-49cd-8c24-743791adedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from flame.core.model.EfficientDetV1.efficientdet import EfficientDet\n",
    "\n",
    "compound_coef = 0\n",
    "\n",
    "model = EfficientDet(\n",
    "    pretrained_weight=f'./checkpoint/efficientdet_pretrained_weight/efficientdet-d{compound_coef}.pth',\n",
    "    head_only=False,\n",
    "    num_classes=20,\n",
    "    compound_coef=compound_coef,\n",
    "    backbone_pretrained=False,\n",
    "    scales=[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)],\n",
    "    aspect_ratios=[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)],\n",
    "    iou_threshold=0.2,\n",
    "    score_threshold=0.2\n",
    ")\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2418cbb-f125-43c1-8e9f-f5bc99a08a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Parameters of Version D0: 3839117 parameters\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f'The Number of Parameters of Version D{compound_coef}: {num_params} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509dba0f-f9b3-4e8f-93c0-782bc95cba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290f88d6-7be2-4a19-9b98-ffbb38c84b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Time Processing:  0.6454s\n",
      "Shape of Input Tensor: torch.Size([2, 3, 512, 512])\n",
      "Shape of Classification Tensor: torch.Size([2, 49104, 20])\n",
      "Shape of Regression Tensor: torch.Size([2, 49104, 4])\n",
      "Shape of Anchors Tensor: torch.Size([1, 49104, 4])\n"
     ]
    }
   ],
   "source": [
    "pascal_iter = iter(pascal_loader)\n",
    "samples, targets, image_infos = pascal_iter.next()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "samples = torch.stack([sample.to(device) for sample in samples], dim=0)\n",
    "\n",
    "t1 = time.time()\n",
    "cls_preds, reg_preds, anchors = model(samples)\n",
    "t2 = time.time()\n",
    "\n",
    "print(f'Forward Time Processing: {t2 - t1: .4f}s')\n",
    "\n",
    "print(f'Shape of Input Tensor: {samples.shape}')  # batch_size x 3 x h x w, (h = w = 512 + 128 * compound_coef)\n",
    "print(f'Shape of Classification Tensor: {cls_preds.shape}')  # batch_size x num_anchors x num_classes\n",
    "print(f'Shape of Regression Tensor: {reg_preds.shape}')  # batch_size x num_anchors x 4\n",
    "print(f'Shape of Anchors Tensor: {anchors.shape}')  # 1 x (w/2^7 * h/2^7 + ... + w/2^3 * h/2^3) * 9 x 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8eddac9-896b-4851-9893-27f11cb7854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49104.0\n"
     ]
    }
   ],
   "source": [
    "h = w = 512 + 128 * compound_coef\n",
    "num_anchor_boxes = sum([(h / 2 ** i) * (w / 2 ** i) * 9 for i in range(3, 8)])  # P3, P4, P5, P6, P7\n",
    "print(num_anchor_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eac498-ca7d-42c3-a4c0-33db8c5ba7d6",
   "metadata": {},
   "source": [
    "## 3. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea32470-a543-4338-abcf-1391588759fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.loss.focal_loss import FocalLoss\n",
    "\n",
    "loss = FocalLoss(\n",
    "    alpha=0.25,\n",
    "    gamma=2.0,\n",
    "    lamda=50.0,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326e7773-c3bb-4a29-a190-dbce95f7ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss, reg_loss = loss(cls_preds, reg_preds, anchors, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1375c4d-847d-4233-85e7-4bbd147071d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14590.7548828125\n"
     ]
    }
   ],
   "source": [
    "loss = cls_loss.mean() + reg_loss.mean()\n",
    "print(f'loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb566519-028b-4014-a39a-12da3fbe8fa8",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4ef7e1-3b67-4db0-a4ed-0a961e0ff45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flame.core.model.EfficientDetV1.efficientdet import EfficientDet\n",
    "\n",
    "compound_coef = 0\n",
    "\n",
    "model = EfficientDet(\n",
    "    pretrained_weight=None,\n",
    "    head_only=False,\n",
    "    num_classes=90,\n",
    "    compound_coef=compound_coef,\n",
    "    backbone_pretrained=False,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.3\n",
    ")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f'./checkpoint/efficientdet_pretrained_weight/efficientdet-d{compound_coef}.pth',\n",
    "        map_location='cpu'\n",
    "    )\n",
    ")\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d7ce2a-ff7a-4eff-909a-dd77c306ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "            'fire hydrant', '', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "            'cow', 'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '', 'handbag', 'tie',\n",
    "            'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "            'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "            'cake', 'chair', 'couch', 'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n",
    "            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "            'refrigerator', '', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950e6462-052b-42ad-a692-93383c00993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "pad_to_square = iaa.PadToSquare(position='right-bottom')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "imsize = 512 + 128 * compound_coef\n",
    "\n",
    "def preprocess(image_paths, imsize=imsize, mean=mean, std=std, device=device):\n",
    "    images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "    padded_images = [pad_to_square(image=image) for image in images]\n",
    "    samples = [cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB) for padded_image in padded_images]\n",
    "    samples = [cv2.resize(sample, dsize=(imsize, imsize)) for sample in samples]\n",
    "    samples = [torch.from_numpy(sample) for sample in samples]\n",
    "    samples = torch.stack(samples, dim=0).to(device)\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = (samples.float().div(255.) - mean) / std\n",
    "    scales = [max(*image.shape) / imsize for image in images]\n",
    "    return images, scales, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ead04d55-c724-4e1f-85d3-8e50c328177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = ['./dataset/VOC2007/valid/000002.jpg',\n",
    "               './dataset/VOC2007/valid/000013.jpg',\n",
    "               './dataset/VOC2007/valid/000030.jpg']\n",
    "\n",
    "images, scales, samples = preprocess(image_paths=image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11f0f487-eb31-4ac9-8f71-4ede20ecd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction time: 0.9432260990142822s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    predictions = model.inference(samples)\n",
    "t2 = time.time()\n",
    "print(f'prediction time: {t2 - t1}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b3dcd3-2164-4d47-b63b-5c081199fa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[143.4083, 203.5088, 211.8467, 306.7638]]),\n",
       "  'labels': tensor([6]),\n",
       "  'scores': tensor([0.7441])},\n",
       " {'boxes': tensor([[303.4525, 163.6749, 457.1618, 257.0464]]),\n",
       "  'labels': tensor([20]),\n",
       "  'scores': tensor([0.8815])},\n",
       " {'boxes': tensor([[300.3644, 137.6334, 460.2697, 296.3701],\n",
       "          [ 56.8792, 164.4981, 152.5491, 292.8531],\n",
       "          [ 28.6209, 205.2891, 182.1070, 296.2229]]),\n",
       "  'labels': tensor([0, 0, 1]),\n",
       "  'scores': tensor([0.9399, 0.9148, 0.7222])}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab3a59d4-8dcc-49bb-83a1-4035ce16f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, scale, pred in zip(images, scales, predictions):\n",
    "    thickness = max(image.shape) // 700\n",
    "    fontscale = max(image.shape) / 800\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    labels = pred['labels'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    class_names = [classes[label] for label in labels]\n",
    "    boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\n",
    "    boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\n",
    "    boxes = boxes.astype(np.int32)\n",
    "    for box, score, class_name in zip(boxes, scores, class_names):\n",
    "        color = (np.random.randint(200, 255),\n",
    "                 np.random.randint(50, 200),\n",
    "                 np.random.randint(0, 150))\n",
    "        if label != -1:\n",
    "            cv2.rectangle(\n",
    "                img=image,\n",
    "                pt1=tuple(box[:2]),\n",
    "                pt2=tuple(box[2:]),    \n",
    "                color=color,\n",
    "                thickness=thickness\n",
    "            )\n",
    "\n",
    "            cv2.putText(\n",
    "                img=image,\n",
    "                text=f'{class_name}: {score: .4f}',\n",
    "                org=tuple(box[:2]),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=fontscale,\n",
    "                color=color,\n",
    "                thickness=thickness,\n",
    "                lineType=cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow(class_name, image)\n",
    "            cv2.waitKey()\n",
    "            cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
