{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be669f2b-4739-44a8-868d-d23199ed3e18",
   "metadata": {},
   "source": [
    "# PASCAL VOC 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649934e1-9849-4c23-a277-4b819d1d8411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "\n",
    "# image_dir = Path('./dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/')\n",
    "# label_dir = Path('./dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/')\n",
    "\n",
    "image_dir = Path('./dataset/PASCALVOC2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/')\n",
    "label_dir = Path('./dataset/PASCALVOC2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/Annotations/')\n",
    "\n",
    "image_pattern = '*.jpg'\n",
    "label_pattern = '*.xml'\n",
    "\n",
    "image_paths = natsorted(image_dir.glob(image_pattern), key=lambda x: x.stem)\n",
    "label_paths = natsorted(label_dir.glob(label_pattern), key=lambda x: x.stem)\n",
    "\n",
    "data_pairs = [(image_path, label_path) for image_path, label_path in zip(image_paths, label_paths) if image_path.stem == label_path.stem]\n",
    "\n",
    "print(len(data_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398d0c2-c5f2-4303-93bc-b7ac3d881e1e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a0324a-ae77-41ee-84ea-31cfe37eb2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "\n",
    "datadir = Path('./dataset/VOC2007/')\n",
    "image_pattern = '*.jpg'\n",
    "label_pattern = '*.xml'\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "for set_name in ['train', 'test', 'valid']:\n",
    "    image_paths = natsorted(datadir.joinpath(set_name).glob(image_pattern), key=lambda x: x.stem)\n",
    "    label_paths = natsorted(datadir.joinpath(set_name).glob(label_pattern), key=lambda x: x.stem)\n",
    "    data_pairs = [(image_path, label_path) for image_path, label_path in zip(image_paths, label_paths) if image_path.stem == label_path.stem]\n",
    "    statistics[set_name] = len(data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b001a84-0dfc-4d5f-9c10-c13be16996aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 6376, 'test': 1993, 'valid': 1594}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75eb92b0-5408-42ca-bf27-59222f714a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, List, Optional\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "\n",
    "\n",
    "class VOC2007Dataset(Dataset):\n",
    "    def __init__(self, datadir: str = None, image_pattern: str = '*.jpg', label_pattern: str = '*.xml', classes: Dict[str, int] = None,\n",
    "                 mean: List[float] = [0.485, 0.456, 0.406], std: List[float] = [0.229, 0.224, 0.225],\n",
    "                 compound_coef: int = 0, transforms: Optional[List] = None):\n",
    "        super(VOC2007Dataset, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.imsize = 512 + compound_coef * 128\n",
    "        self.std = torch.tensor(std, dtype=torch.float).view(3, 1, 1)\n",
    "        self.mean = torch.tensor(mean, dtype=torch.float).view(3, 1, 1)\n",
    "\n",
    "        self.transforms = transforms if transforms else []\n",
    "\n",
    "        image_paths = natsorted(list(Path(datadir).glob(f'{image_pattern}')), key=lambda x: str(x.stem))\n",
    "        label_paths = natsorted(list(Path(datadir).glob(f'{label_pattern}')), key=lambda x: str(x.stem))\n",
    "\n",
    "        self.data_pairs = [[image, label] for image, label in zip(image_paths, label_paths)]\n",
    "        print(f'{Path(datadir).stem}: {len(self.data_pairs)}')\n",
    "\n",
    "        self.pad_to_square = iaa.PadToSquare(position='right-bottom')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def _get_label_info(self, label_path):\n",
    "        tree = ET.parse(str(label_path))\n",
    "        image_info = {'image_name': tree.find('filename').text,\n",
    "                      'height': int(tree.find('size').find('height').text),\n",
    "                      'width': int(tree.find('size').find('width').text),\n",
    "                      'depth': int(tree.find('size').find('depth').text)}\n",
    "        label_info = []\n",
    "        objects = tree.findall('object')\n",
    "        for obj in objects:\n",
    "            bndbox = obj.find('bndbox')\n",
    "            bbox = np.int32([bndbox.find('xmin').text, bndbox.find('ymin').text,\n",
    "                             bndbox.find('xmax').text, bndbox.find('ymax').text])\n",
    "            label_name = obj.find('name').text\n",
    "            label_info.append({'label': label_name, 'bbox': bbox})\n",
    "\n",
    "        return image_info, label_info\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label_path = self.data_pairs[idx]\n",
    "        _, label_info = self._get_label_info(label_path)\n",
    "\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image_info = [str(image_path), image.shape[1::-1]]\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes = [label['bbox'] for label in label_info]\n",
    "        labels = [self.classes[label['label']] for label in label_info]\n",
    "\n",
    "        # Pad to square to keep object's ratio\n",
    "        bbs = BoundingBoxesOnImage([BoundingBox(x1=box[0], y1=box[1], x2=box[2], y2=box[3], label=label)\n",
    "                                    for box, label in zip(boxes, labels)], shape=image.shape)\n",
    "        for transform in random.sample(self.transforms, k=random.randint(0, len(self.transforms))):\n",
    "            image, bbs = transform(image=image, bounding_boxes=bbs)\n",
    "\n",
    "        # Rescale image and bounding boxes\n",
    "        image, bbs = self.pad_to_square(image=image, bounding_boxes=bbs)\n",
    "        sample, bbs = iaa.Resize(size=self.imsize)(image=image, bounding_boxes=bbs)\n",
    "        bbs = bbs.on(sample)\n",
    "\n",
    "        # Convert from Bouding Box Object to boxes, labels list\n",
    "        boxes = [[bb.x1, bb.y1, bb.x2, bb.y2] for bb in bbs.bounding_boxes]\n",
    "        labels = [bb.label for bb in bbs.bounding_boxes]\n",
    "\n",
    "        # Convert to Torch Tensor\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        image_id = torch.tensor([idx], dtype=torch.int64)\n",
    "\n",
    "        # # Target\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': image_id}\n",
    "\n",
    "        # Image\n",
    "        sample = torch.from_numpy(np.ascontiguousarray(sample))\n",
    "        sample = sample.permute(2, 0, 1).contiguous()\n",
    "        sample = (sample.float().div(255.) - self.mean) / self.std\n",
    "\n",
    "        return sample, target, image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e16cd81-4084-410c-817b-fe8c73b5b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 6376\n",
      "number of dataset: 6376\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes2idx = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "               'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "               'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "               'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "\n",
    "transforms = [iaa.Add(value=(-10, 10), per_channel=True),\n",
    "              iaa.GaussianBlur(sigma=(0, 1)),\n",
    "              iaa.MotionBlur(),\n",
    "              iaa.JpegCompression(compression=(0, 10)),\n",
    "              iaa.Fliplr(p=0.5),\n",
    "              iaa.Flipud(p=0.5),\n",
    "              iaa.Grayscale(alpha=(0.0, 0.1)),\n",
    "#               iaa.Rot90(k=[0, 1, 2, 3], keep_size=False),\n",
    "              iaa.Affine(rotate=(-5, 5), shear=(-5, 5), fit_output=True),\n",
    "              iaa.Crop(percent=(0, 0.1)),\n",
    "              iaa.Pad(percent=(0, 0.1), keep_size=False),\n",
    "              iaa.ChangeColorTemperature()]\n",
    "\n",
    "voc2007_dataset = VOC2007Dataset(datadir='./dataset/VOC2007/train/', image_pattern='*.jpg', label_pattern='*.xml',\n",
    "                                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], compound_coef=0,\n",
    "                                 classes=classes2idx, transforms=transforms)\n",
    "\n",
    "print(f'number of dataset: {len(voc2007_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d59d25-85b9-4f99-83c1-fe786308ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_loader = DataLoader(voc2007_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float).view(3, 1, 1)\n",
    "idx2class = {idx: label_name for label_name, idx in classes2idx.items()}\n",
    "\n",
    "for i, voc_data in enumerate(iter(voc_loader)):\n",
    "    samples, targets, sample_infos = voc_data\n",
    "    for sample, target in zip(samples, targets):\n",
    "        image = ((sample * std + mean) * 255).permute(1, 2, 0).to(torch.uint8).cpu().numpy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        boxes = target['boxes'].data.cpu().numpy().astype(np.int32)\n",
    "        labels = target['labels'].data.cpu().numpy().astype(np.int32)\n",
    "        for box, label in zip(boxes, labels):\n",
    "            if label != -1:\n",
    "                image = np.ascontiguousarray(image)\n",
    "                cv2.rectangle(img=image, pt1=(int(box[0]), int(box[1])), pt2=(int(box[2]), int(box[3])),\n",
    "                              color=(0, 255, 0), thickness=1)\n",
    "                cv2.putText(img=image, text=idx2class[int(label)], org=tuple(box[:2]),\n",
    "                            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.001 * max(image.shape[0], image.shape[1]),\n",
    "                            color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('image', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cdf72-6115-4a4b-b5cf-a09ada3581cb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3808523-ae0c-49cd-8c24-743791adedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flame.core.model.efficientdet import EfficientDet\n",
    "\n",
    "pretrained_weight = './checkpoint/efficientdet_pretrained_weight/efficientdet-d1.pth'\n",
    "head_only = False\n",
    "num_classes = 20\n",
    "compound_coef = 1\n",
    "backbone_pretrained = False\n",
    "scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n",
    "aspect_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n",
    "iou_threshold = 0.2\n",
    "score_threshold = 0.2\n",
    "\n",
    "model = EfficientDet(pretrained_weight=pretrained_weight,\n",
    "                     head_only=head_only,\n",
    "                     num_classes=num_classes,\n",
    "                     compound_coef=compound_coef,\n",
    "                     backbone_pretrained=backbone_pretrained,\n",
    "                     scales=scales, aspect_ratios=aspect_ratios,\n",
    "                     iou_threshold=iou_threshold,\n",
    "                     score_threshold=score_threshold)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2418cbb-f125-43c1-8e9f-f5bc99a08a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6569828\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509dba0f-f9b3-4e8f-93c0-782bc95cba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+------------+\n",
      "|                             Modules                             | Parameters |\n",
      "+-----------------------------------------------------------------+------------+\n",
      "|                       model.bifpn.0.p6_w1                       |     2      |\n",
      "|                       model.bifpn.0.p5_w1                       |     2      |\n",
      "|                       model.bifpn.0.p4_w1                       |     2      |\n",
      "|                       model.bifpn.0.p3_w1                       |     2      |\n",
      "|                       model.bifpn.0.p4_w2                       |     3      |\n",
      "|                       model.bifpn.0.p5_w2                       |     3      |\n",
      "|                       model.bifpn.0.p6_w2                       |     3      |\n",
      "|                       model.bifpn.0.p7_w2                       |     2      |\n",
      "|        model.bifpn.0.conv6_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv6_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv6_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv6_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv6_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.0.conv5_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv5_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv5_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv5_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv5_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.0.conv4_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv4_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv4_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv4_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv4_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.0.conv3_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv3_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv3_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv3_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv3_up.bn.bias                 |     64     |\n",
      "|       model.bifpn.0.conv4_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv4_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv4_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv4_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv4_down.bn.bias                |     64     |\n",
      "|       model.bifpn.0.conv5_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv5_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv5_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv5_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv5_down.bn.bias                |     64     |\n",
      "|       model.bifpn.0.conv6_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv6_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv6_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv6_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv6_down.bn.bias                |     64     |\n",
      "|       model.bifpn.0.conv7_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv7_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv7_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv7_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv7_down.bn.bias                |     64     |\n",
      "|           model.bifpn.0.p5_down_channel.0.conv.weight           |   20480    |\n",
      "|            model.bifpn.0.p5_down_channel.0.conv.bias            |     64     |\n",
      "|              model.bifpn.0.p5_down_channel.1.weight             |     64     |\n",
      "|               model.bifpn.0.p5_down_channel.1.bias              |     64     |\n",
      "|           model.bifpn.0.p4_down_channel.0.conv.weight           |    7168    |\n",
      "|            model.bifpn.0.p4_down_channel.0.conv.bias            |     64     |\n",
      "|              model.bifpn.0.p4_down_channel.1.weight             |     64     |\n",
      "|               model.bifpn.0.p4_down_channel.1.bias              |     64     |\n",
      "|           model.bifpn.0.p3_down_channel.0.conv.weight           |    2560    |\n",
      "|            model.bifpn.0.p3_down_channel.0.conv.bias            |     64     |\n",
      "|              model.bifpn.0.p3_down_channel.1.weight             |     64     |\n",
      "|               model.bifpn.0.p3_down_channel.1.bias              |     64     |\n",
      "|               model.bifpn.0.p5_to_p6.0.conv.weight              |   20480    |\n",
      "|                model.bifpn.0.p5_to_p6.0.conv.bias               |     64     |\n",
      "|                 model.bifpn.0.p5_to_p6.1.weight                 |     64     |\n",
      "|                  model.bifpn.0.p5_to_p6.1.bias                  |     64     |\n",
      "|          model.bifpn.0.p4_down_channel_2.0.conv.weight          |    7168    |\n",
      "|           model.bifpn.0.p4_down_channel_2.0.conv.bias           |     64     |\n",
      "|             model.bifpn.0.p4_down_channel_2.1.weight            |     64     |\n",
      "|              model.bifpn.0.p4_down_channel_2.1.bias             |     64     |\n",
      "|          model.bifpn.0.p5_down_channel_2.0.conv.weight          |   20480    |\n",
      "|           model.bifpn.0.p5_down_channel_2.0.conv.bias           |     64     |\n",
      "|             model.bifpn.0.p5_down_channel_2.1.weight            |     64     |\n",
      "|              model.bifpn.0.p5_down_channel_2.1.bias             |     64     |\n",
      "|                       model.bifpn.1.p6_w1                       |     2      |\n",
      "|                       model.bifpn.1.p5_w1                       |     2      |\n",
      "|                       model.bifpn.1.p4_w1                       |     2      |\n",
      "|                       model.bifpn.1.p3_w1                       |     2      |\n",
      "|                       model.bifpn.1.p4_w2                       |     3      |\n",
      "|                       model.bifpn.1.p5_w2                       |     3      |\n",
      "|                       model.bifpn.1.p6_w2                       |     3      |\n",
      "|                       model.bifpn.1.p7_w2                       |     2      |\n",
      "|        model.bifpn.1.conv6_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv6_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv6_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv6_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv6_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.1.conv5_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv5_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv5_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv5_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv5_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.1.conv4_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv4_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv4_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv4_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv4_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.1.conv3_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv3_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv3_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv3_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv3_up.bn.bias                 |     64     |\n",
      "|       model.bifpn.1.conv4_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv4_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv4_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv4_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv4_down.bn.bias                |     64     |\n",
      "|       model.bifpn.1.conv5_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv5_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv5_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv5_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv5_down.bn.bias                |     64     |\n",
      "|       model.bifpn.1.conv6_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv6_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv6_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv6_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv6_down.bn.bias                |     64     |\n",
      "|       model.bifpn.1.conv7_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv7_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv7_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv7_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv7_down.bn.bias                |     64     |\n",
      "|                       model.bifpn.2.p6_w1                       |     2      |\n",
      "|                       model.bifpn.2.p5_w1                       |     2      |\n",
      "|                       model.bifpn.2.p4_w1                       |     2      |\n",
      "|                       model.bifpn.2.p3_w1                       |     2      |\n",
      "|                       model.bifpn.2.p4_w2                       |     3      |\n",
      "|                       model.bifpn.2.p5_w2                       |     3      |\n",
      "|                       model.bifpn.2.p6_w2                       |     3      |\n",
      "|                       model.bifpn.2.p7_w2                       |     2      |\n",
      "|        model.bifpn.2.conv6_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv6_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv6_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv6_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv6_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.2.conv5_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv5_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv5_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv5_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv5_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.2.conv4_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv4_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv4_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv4_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv4_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.2.conv3_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv3_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv3_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv3_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv3_up.bn.bias                 |     64     |\n",
      "|       model.bifpn.2.conv4_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv4_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv4_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv4_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv4_down.bn.bias                |     64     |\n",
      "|       model.bifpn.2.conv5_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv5_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv5_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv5_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv5_down.bn.bias                |     64     |\n",
      "|       model.bifpn.2.conv6_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv6_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv6_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv6_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv6_down.bn.bias                |     64     |\n",
      "|       model.bifpn.2.conv7_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv7_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv7_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv7_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv7_down.bn.bias                |     64     |\n",
      "|      model.regressor.conv_list.0.depthwise_conv.conv.weight     |    576     |\n",
      "|      model.regressor.conv_list.0.pointwise_conv.conv.weight     |    4096    |\n",
      "|       model.regressor.conv_list.0.pointwise_conv.conv.bias      |     64     |\n",
      "|      model.regressor.conv_list.1.depthwise_conv.conv.weight     |    576     |\n",
      "|      model.regressor.conv_list.1.pointwise_conv.conv.weight     |    4096    |\n",
      "|       model.regressor.conv_list.1.pointwise_conv.conv.bias      |     64     |\n",
      "|      model.regressor.conv_list.2.depthwise_conv.conv.weight     |    576     |\n",
      "|      model.regressor.conv_list.2.pointwise_conv.conv.weight     |    4096    |\n",
      "|       model.regressor.conv_list.2.pointwise_conv.conv.bias      |     64     |\n",
      "|                model.regressor.bn_list.0.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.0.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.0.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.0.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.0.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.0.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.1.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.1.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.1.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.1.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.1.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.1.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.2.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.2.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.2.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.2.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.2.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.2.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.3.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.3.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.3.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.3.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.3.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.3.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.4.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.4.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.4.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.4.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.4.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.4.2.bias                |     64     |\n",
      "|        model.regressor.header.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.regressor.header.pointwise_conv.conv.weight        |    2304    |\n",
      "|         model.regressor.header.pointwise_conv.conv.bias         |     36     |\n",
      "|     model.classifier.conv_list.0.depthwise_conv.conv.weight     |    576     |\n",
      "|     model.classifier.conv_list.0.pointwise_conv.conv.weight     |    4096    |\n",
      "|      model.classifier.conv_list.0.pointwise_conv.conv.bias      |     64     |\n",
      "|     model.classifier.conv_list.1.depthwise_conv.conv.weight     |    576     |\n",
      "|     model.classifier.conv_list.1.pointwise_conv.conv.weight     |    4096    |\n",
      "|      model.classifier.conv_list.1.pointwise_conv.conv.bias      |     64     |\n",
      "|     model.classifier.conv_list.2.depthwise_conv.conv.weight     |    576     |\n",
      "|     model.classifier.conv_list.2.pointwise_conv.conv.weight     |    4096    |\n",
      "|      model.classifier.conv_list.2.pointwise_conv.conv.bias      |     64     |\n",
      "|               model.classifier.bn_list.0.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.0.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.0.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.0.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.0.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.0.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.1.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.1.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.1.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.1.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.1.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.1.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.2.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.2.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.2.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.2.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.2.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.2.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.3.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.3.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.3.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.3.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.3.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.3.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.4.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.4.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.4.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.4.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.4.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.4.2.bias                |     64     |\n",
      "|        model.classifier.header.depthwise_conv.conv.weight       |    576     |\n",
      "|        model.classifier.header.pointwise_conv.conv.weight       |   11520    |\n",
      "|         model.classifier.header.pointwise_conv.conv.bias        |    180     |\n",
      "|         model.backbone_net.model._conv_stem.conv.weight         |    864     |\n",
      "|               model.backbone_net.model._bn0.weight              |     32     |\n",
      "|                model.backbone_net.model._bn0.bias               |     32     |\n",
      "|  model.backbone_net.model._blocks.0._depthwise_conv.conv.weight |    288     |\n",
      "|          model.backbone_net.model._blocks.0._bn1.weight         |     32     |\n",
      "|           model.backbone_net.model._blocks.0._bn1.bias          |     32     |\n",
      "|    model.backbone_net.model._blocks.0._se_reduce.conv.weight    |    256     |\n",
      "|     model.backbone_net.model._blocks.0._se_reduce.conv.bias     |     8      |\n",
      "|    model.backbone_net.model._blocks.0._se_expand.conv.weight    |    256     |\n",
      "|     model.backbone_net.model._blocks.0._se_expand.conv.bias     |     32     |\n",
      "|   model.backbone_net.model._blocks.0._project_conv.conv.weight  |    512     |\n",
      "|          model.backbone_net.model._blocks.0._bn2.weight         |     16     |\n",
      "|           model.backbone_net.model._blocks.0._bn2.bias          |     16     |\n",
      "|   model.backbone_net.model._blocks.1._expand_conv.conv.weight   |    1536    |\n",
      "|          model.backbone_net.model._blocks.1._bn0.weight         |     96     |\n",
      "|           model.backbone_net.model._blocks.1._bn0.bias          |     96     |\n",
      "|  model.backbone_net.model._blocks.1._depthwise_conv.conv.weight |    864     |\n",
      "|          model.backbone_net.model._blocks.1._bn1.weight         |     96     |\n",
      "|           model.backbone_net.model._blocks.1._bn1.bias          |     96     |\n",
      "|    model.backbone_net.model._blocks.1._se_reduce.conv.weight    |    384     |\n",
      "|     model.backbone_net.model._blocks.1._se_reduce.conv.bias     |     4      |\n",
      "|    model.backbone_net.model._blocks.1._se_expand.conv.weight    |    384     |\n",
      "|     model.backbone_net.model._blocks.1._se_expand.conv.bias     |     96     |\n",
      "|   model.backbone_net.model._blocks.1._project_conv.conv.weight  |    2304    |\n",
      "|          model.backbone_net.model._blocks.1._bn2.weight         |     24     |\n",
      "|           model.backbone_net.model._blocks.1._bn2.bias          |     24     |\n",
      "|   model.backbone_net.model._blocks.2._expand_conv.conv.weight   |    3456    |\n",
      "|          model.backbone_net.model._blocks.2._bn0.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.2._bn0.bias          |    144     |\n",
      "|  model.backbone_net.model._blocks.2._depthwise_conv.conv.weight |    1296    |\n",
      "|          model.backbone_net.model._blocks.2._bn1.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.2._bn1.bias          |    144     |\n",
      "|    model.backbone_net.model._blocks.2._se_reduce.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.2._se_reduce.conv.bias     |     6      |\n",
      "|    model.backbone_net.model._blocks.2._se_expand.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.2._se_expand.conv.bias     |    144     |\n",
      "|   model.backbone_net.model._blocks.2._project_conv.conv.weight  |    3456    |\n",
      "|          model.backbone_net.model._blocks.2._bn2.weight         |     24     |\n",
      "|           model.backbone_net.model._blocks.2._bn2.bias          |     24     |\n",
      "|   model.backbone_net.model._blocks.3._expand_conv.conv.weight   |    3456    |\n",
      "|          model.backbone_net.model._blocks.3._bn0.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.3._bn0.bias          |    144     |\n",
      "|  model.backbone_net.model._blocks.3._depthwise_conv.conv.weight |    3600    |\n",
      "|          model.backbone_net.model._blocks.3._bn1.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.3._bn1.bias          |    144     |\n",
      "|    model.backbone_net.model._blocks.3._se_reduce.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.3._se_reduce.conv.bias     |     6      |\n",
      "|    model.backbone_net.model._blocks.3._se_expand.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.3._se_expand.conv.bias     |    144     |\n",
      "|   model.backbone_net.model._blocks.3._project_conv.conv.weight  |    5760    |\n",
      "|          model.backbone_net.model._blocks.3._bn2.weight         |     40     |\n",
      "|           model.backbone_net.model._blocks.3._bn2.bias          |     40     |\n",
      "|   model.backbone_net.model._blocks.4._expand_conv.conv.weight   |    9600    |\n",
      "|          model.backbone_net.model._blocks.4._bn0.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.4._bn0.bias          |    240     |\n",
      "|  model.backbone_net.model._blocks.4._depthwise_conv.conv.weight |    6000    |\n",
      "|          model.backbone_net.model._blocks.4._bn1.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.4._bn1.bias          |    240     |\n",
      "|    model.backbone_net.model._blocks.4._se_reduce.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.4._se_reduce.conv.bias     |     10     |\n",
      "|    model.backbone_net.model._blocks.4._se_expand.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.4._se_expand.conv.bias     |    240     |\n",
      "|   model.backbone_net.model._blocks.4._project_conv.conv.weight  |    9600    |\n",
      "|          model.backbone_net.model._blocks.4._bn2.weight         |     40     |\n",
      "|           model.backbone_net.model._blocks.4._bn2.bias          |     40     |\n",
      "|   model.backbone_net.model._blocks.5._expand_conv.conv.weight   |    9600    |\n",
      "|          model.backbone_net.model._blocks.5._bn0.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.5._bn0.bias          |    240     |\n",
      "|  model.backbone_net.model._blocks.5._depthwise_conv.conv.weight |    2160    |\n",
      "|          model.backbone_net.model._blocks.5._bn1.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.5._bn1.bias          |    240     |\n",
      "|    model.backbone_net.model._blocks.5._se_reduce.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.5._se_reduce.conv.bias     |     10     |\n",
      "|    model.backbone_net.model._blocks.5._se_expand.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.5._se_expand.conv.bias     |    240     |\n",
      "|   model.backbone_net.model._blocks.5._project_conv.conv.weight  |   19200    |\n",
      "|          model.backbone_net.model._blocks.5._bn2.weight         |     80     |\n",
      "|           model.backbone_net.model._blocks.5._bn2.bias          |     80     |\n",
      "|   model.backbone_net.model._blocks.6._expand_conv.conv.weight   |   38400    |\n",
      "|          model.backbone_net.model._blocks.6._bn0.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.6._bn0.bias          |    480     |\n",
      "|  model.backbone_net.model._blocks.6._depthwise_conv.conv.weight |    4320    |\n",
      "|          model.backbone_net.model._blocks.6._bn1.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.6._bn1.bias          |    480     |\n",
      "|    model.backbone_net.model._blocks.6._se_reduce.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.6._se_reduce.conv.bias     |     20     |\n",
      "|    model.backbone_net.model._blocks.6._se_expand.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.6._se_expand.conv.bias     |    480     |\n",
      "|   model.backbone_net.model._blocks.6._project_conv.conv.weight  |   38400    |\n",
      "|          model.backbone_net.model._blocks.6._bn2.weight         |     80     |\n",
      "|           model.backbone_net.model._blocks.6._bn2.bias          |     80     |\n",
      "|   model.backbone_net.model._blocks.7._expand_conv.conv.weight   |   38400    |\n",
      "|          model.backbone_net.model._blocks.7._bn0.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.7._bn0.bias          |    480     |\n",
      "|  model.backbone_net.model._blocks.7._depthwise_conv.conv.weight |    4320    |\n",
      "|          model.backbone_net.model._blocks.7._bn1.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.7._bn1.bias          |    480     |\n",
      "|    model.backbone_net.model._blocks.7._se_reduce.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.7._se_reduce.conv.bias     |     20     |\n",
      "|    model.backbone_net.model._blocks.7._se_expand.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.7._se_expand.conv.bias     |    480     |\n",
      "|   model.backbone_net.model._blocks.7._project_conv.conv.weight  |   38400    |\n",
      "|          model.backbone_net.model._blocks.7._bn2.weight         |     80     |\n",
      "|           model.backbone_net.model._blocks.7._bn2.bias          |     80     |\n",
      "|   model.backbone_net.model._blocks.8._expand_conv.conv.weight   |   38400    |\n",
      "|          model.backbone_net.model._blocks.8._bn0.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.8._bn0.bias          |    480     |\n",
      "|  model.backbone_net.model._blocks.8._depthwise_conv.conv.weight |   12000    |\n",
      "|          model.backbone_net.model._blocks.8._bn1.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.8._bn1.bias          |    480     |\n",
      "|    model.backbone_net.model._blocks.8._se_reduce.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.8._se_reduce.conv.bias     |     20     |\n",
      "|    model.backbone_net.model._blocks.8._se_expand.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.8._se_expand.conv.bias     |    480     |\n",
      "|   model.backbone_net.model._blocks.8._project_conv.conv.weight  |   53760    |\n",
      "|          model.backbone_net.model._blocks.8._bn2.weight         |    112     |\n",
      "|           model.backbone_net.model._blocks.8._bn2.bias          |    112     |\n",
      "|   model.backbone_net.model._blocks.9._expand_conv.conv.weight   |   75264    |\n",
      "|          model.backbone_net.model._blocks.9._bn0.weight         |    672     |\n",
      "|           model.backbone_net.model._blocks.9._bn0.bias          |    672     |\n",
      "|  model.backbone_net.model._blocks.9._depthwise_conv.conv.weight |   16800    |\n",
      "|          model.backbone_net.model._blocks.9._bn1.weight         |    672     |\n",
      "|           model.backbone_net.model._blocks.9._bn1.bias          |    672     |\n",
      "|    model.backbone_net.model._blocks.9._se_reduce.conv.weight    |   18816    |\n",
      "|     model.backbone_net.model._blocks.9._se_reduce.conv.bias     |     28     |\n",
      "|    model.backbone_net.model._blocks.9._se_expand.conv.weight    |   18816    |\n",
      "|     model.backbone_net.model._blocks.9._se_expand.conv.bias     |    672     |\n",
      "|   model.backbone_net.model._blocks.9._project_conv.conv.weight  |   75264    |\n",
      "|          model.backbone_net.model._blocks.9._bn2.weight         |    112     |\n",
      "|           model.backbone_net.model._blocks.9._bn2.bias          |    112     |\n",
      "|   model.backbone_net.model._blocks.10._expand_conv.conv.weight  |   75264    |\n",
      "|         model.backbone_net.model._blocks.10._bn0.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.10._bn0.bias          |    672     |\n",
      "| model.backbone_net.model._blocks.10._depthwise_conv.conv.weight |   16800    |\n",
      "|         model.backbone_net.model._blocks.10._bn1.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.10._bn1.bias          |    672     |\n",
      "|    model.backbone_net.model._blocks.10._se_reduce.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.10._se_reduce.conv.bias    |     28     |\n",
      "|    model.backbone_net.model._blocks.10._se_expand.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.10._se_expand.conv.bias    |    672     |\n",
      "|  model.backbone_net.model._blocks.10._project_conv.conv.weight  |   75264    |\n",
      "|         model.backbone_net.model._blocks.10._bn2.weight         |    112     |\n",
      "|          model.backbone_net.model._blocks.10._bn2.bias          |    112     |\n",
      "|   model.backbone_net.model._blocks.11._expand_conv.conv.weight  |   75264    |\n",
      "|         model.backbone_net.model._blocks.11._bn0.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.11._bn0.bias          |    672     |\n",
      "| model.backbone_net.model._blocks.11._depthwise_conv.conv.weight |   16800    |\n",
      "|         model.backbone_net.model._blocks.11._bn1.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.11._bn1.bias          |    672     |\n",
      "|    model.backbone_net.model._blocks.11._se_reduce.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.11._se_reduce.conv.bias    |     28     |\n",
      "|    model.backbone_net.model._blocks.11._se_expand.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.11._se_expand.conv.bias    |    672     |\n",
      "|  model.backbone_net.model._blocks.11._project_conv.conv.weight  |   129024   |\n",
      "|         model.backbone_net.model._blocks.11._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.11._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.12._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.12._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.12._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.12._depthwise_conv.conv.weight |   28800    |\n",
      "|         model.backbone_net.model._blocks.12._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.12._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.12._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.12._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.12._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.12._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.12._project_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.12._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.12._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.13._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.13._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.13._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.13._depthwise_conv.conv.weight |   28800    |\n",
      "|         model.backbone_net.model._blocks.13._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.13._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.13._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.13._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.13._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.13._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.13._project_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.13._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.13._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.14._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.14._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.14._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.14._depthwise_conv.conv.weight |   28800    |\n",
      "|         model.backbone_net.model._blocks.14._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.14._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.14._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.14._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.14._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.14._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.14._project_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.14._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.14._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.15._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.15._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.15._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.15._depthwise_conv.conv.weight |   10368    |\n",
      "|         model.backbone_net.model._blocks.15._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.15._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.15._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.15._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.15._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.15._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.15._project_conv.conv.weight  |   368640   |\n",
      "|         model.backbone_net.model._blocks.15._bn2.weight         |    320     |\n",
      "|          model.backbone_net.model._blocks.15._bn2.bias          |    320     |\n",
      "+-----------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 3839117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3839117"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "290f88d6-7be2-4a19-9b98-ffbb38c84b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_iter = iter(voc_loader)\n",
    "samples, targets, image_infos = voc_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d98da99-b16a-42c0-861e-cb6d8cc0bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 49104, 20])\n",
      "torch.Size([2, 49104, 4])\n",
      "torch.Size([1, 49104, 4])\n"
     ]
    }
   ],
   "source": [
    "cls_preds, reg_preds, anchors = model(samples)\n",
    "print(cls_preds.shape)\n",
    "print(reg_preds.shape)\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eac498-ca7d-42c3-a4c0-33db8c5ba7d6",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea32470-a543-4338-abcf-1391588759fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.loss.focal_loss import FocalLoss\n",
    "\n",
    "loss = FocalLoss(alpha=0.25, gamma=2.0, lamda=50.0, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "326e7773-c3bb-4a29-a190-dbce95f7ee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10546.2178], grad_fn=<MeanBackward1>),\n",
       " tensor([3.1572], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(cls_preds, reg_preds, anchors, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb566519-028b-4014-a39a-12da3fbe8fa8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4ef7e1-3b67-4db0-a4ed-0a961e0ff45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.model.efficientdet import EfficientDet\n",
    "\n",
    "compound_coef = 0\n",
    "\n",
    "model = EfficientDet(pretrained_weight=None,\n",
    "                     head_only=False,\n",
    "                     num_classes=90,\n",
    "                     compound_coef=compound_coef,\n",
    "                     backbone_pretrained=False,\n",
    "                     score_threshold=0.3)\n",
    "model.load_state_dict(torch.load(f'./checkpoint/efficientdet_pretrained_weight/efficientdet-d{compound_coef}.pth', map_location='cpu'))\n",
    "# model.requires_grad_(False)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d7ce2a-ff7a-4eff-909a-dd77c306ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "            'fire hydrant', '', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "            'cow', 'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '', 'handbag', 'tie',\n",
    "            'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "            'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "            'cake', 'chair', 'couch', 'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n",
    "            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "            'refrigerator', '', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950e6462-052b-42ad-a692-93383c00993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "pad_to_square = iaa.PadToSquare(position='right-bottom')\n",
    "device = 'cpu'\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float).view(1, 3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float).view(1, 3, 1, 1)\n",
    "\n",
    "imsize = 512 + 128 * compound_coef\n",
    "\n",
    "def preprocess(image_paths, imsize=imsize, mean=mean, std=std, device=device):\n",
    "    images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "    padded_images = [pad_to_square(image=image) for image in images]\n",
    "    samples = [cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB) for padded_image in padded_images]\n",
    "    samples = [cv2.resize(sample, dsize=(imsize, imsize)) for sample in samples]\n",
    "    samples = [torch.from_numpy(sample) for sample in samples]\n",
    "    samples = torch.stack(samples, dim=0).to(device)\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = (samples.float().div(255.) - mean) / std\n",
    "    scales = [max(*image.shape) / imsize for image in images]\n",
    "    return images, scales, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ead04d55-c724-4e1f-85d3-8e50c328177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = ['./dataset/VOC2007/valid/000002.jpg',\n",
    "#                './dataset/VOC2007/valid/000013.jpg',\n",
    "#                './dataset/VOC2007/valid/000030.jpg']\n",
    "image_paths = ['./dataset/VOC2007/valid/000002.jpg']\n",
    "images, scales, samples = preprocess(image_paths=image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f0f487-eb31-4ac9-8f71-4ede20ecd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction time: 0.3552844524383545s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    predictions = model(samples)\n",
    "t2 = time.time()\n",
    "print(f'prediction time: {t2 - t1}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26b3dcd3-2164-4d47-b63b-5c081199fa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[143.4082, 203.5088, 211.8467, 306.7638]]),\n",
       "  'labels': tensor([6]),\n",
       "  'scores': tensor([0.7441])}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab3a59d4-8dcc-49bb-83a1-4035ce16f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, scale, pred in zip(images, scales, predictions):\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    labels = pred['labels'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    class_names = [classes[label] for label in labels]\n",
    "    boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\n",
    "    boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\n",
    "    boxes = boxes.astype(np.int32)\n",
    "    for box, score, class_name in zip(boxes, scores, class_names):\n",
    "        color = (np.random.randint(200, 255),\n",
    "                 np.random.randint(50, 200),\n",
    "                 np.random.randint(0, 150))\n",
    "        cv2.putText(image,\n",
    "                    text=f'{class_name}',\n",
    "                    org=tuple(box[:2]),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.001 * max(image.shape[0], image.shape[1]),\n",
    "                    color=color,\n",
    "                    thickness=1,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "        cv2.rectangle(image, pt1=tuple(box[:2]), pt2=tuple(box[2:]), color=color, thickness=3)\n",
    "        cv2.imshow('a', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
