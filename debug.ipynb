{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be669f2b-4739-44a8-868d-d23199ed3e18",
   "metadata": {},
   "source": [
    "# PASCAL VOC 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649934e1-9849-4c23-a277-4b819d1d8411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "\n",
    "# image_dir = Path('./dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/')\n",
    "# label_dir = Path('./dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/')\n",
    "\n",
    "image_dir = Path('./dataset/PASCALVOC2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/')\n",
    "label_dir = Path('./dataset/PASCALVOC2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/Annotations/')\n",
    "\n",
    "image_pattern = '*.jpg'\n",
    "label_pattern = '*.xml'\n",
    "\n",
    "image_paths = natsorted(image_dir.glob(image_pattern), key=lambda x: x.stem)\n",
    "label_paths = natsorted(label_dir.glob(label_pattern), key=lambda x: x.stem)\n",
    "\n",
    "data_pairs = [(image_path, label_path) for image_path, label_path in zip(image_paths, label_paths) if image_path.stem == label_path.stem]\n",
    "\n",
    "print(len(data_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398d0c2-c5f2-4303-93bc-b7ac3d881e1e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a0324a-ae77-41ee-84ea-31cfe37eb2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 6376, 'test': 1993, 'valid': 1594}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "\n",
    "datadir = Path('./dataset/VOC2007/')\n",
    "image_pattern = '*.jpg'\n",
    "label_pattern = '*.xml'\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "for set_name in ['train', 'test', 'valid']:\n",
    "    image_paths = natsorted(datadir.joinpath(set_name).glob(image_pattern), key=lambda x: x.stem)\n",
    "    label_paths = natsorted(datadir.joinpath(set_name).glob(label_pattern), key=lambda x: x.stem)\n",
    "    data_pairs = [(image_path, label_path) for image_path, label_path in zip(image_paths, label_paths) if image_path.stem == label_path.stem]\n",
    "    statistics[set_name] = len(data_pairs)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e16cd81-4084-410c-817b-fe8c73b5b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPEGImages: 4952\n",
      "number of dataset: 4952\n"
     ]
    }
   ],
   "source": [
    "from flame.core.data.voc2007_dataset import VOC2007Dataset\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes2idx = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "               'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "               'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "               'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "\n",
    "transforms = [iaa.Add(value=(-10, 10), per_channel=True),\n",
    "              iaa.GaussianBlur(sigma=(0, 1)),\n",
    "              iaa.MotionBlur(),\n",
    "              iaa.JpegCompression(compression=(0, 10)),\n",
    "              iaa.Fliplr(p=0.5),\n",
    "              iaa.Flipud(p=0.5),\n",
    "              iaa.Grayscale(alpha=(0.0, 0.1)),\n",
    "#               iaa.Rot90(k=[0, 1, 2, 3], keep_size=False),\n",
    "              iaa.Affine(rotate=(-5, 5), shear=(-5, 5), fit_output=True),\n",
    "              iaa.Crop(percent=(0, 0.1)),\n",
    "              iaa.Pad(percent=(0, 0.1), keep_size=False),\n",
    "              iaa.ChangeColorTemperature()]\n",
    "\n",
    "image_dir = './dataset/PASCALVOC2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/'\n",
    "label_dir = './dataset/PASCALVOC2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/Annotations/'\n",
    "\n",
    "voc2007_dataset = VOC2007Dataset(image_dir=image_dir, label_dir=label_dir,\n",
    "                                 image_pattern='*.jpg', label_pattern='*.xml',\n",
    "                                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], compound_coef=0,\n",
    "                                 classes=classes2idx, transforms=transforms)\n",
    "\n",
    "print(f'number of dataset: {len(voc2007_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d59d25-85b9-4f99-83c1-fe786308ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_loader = DataLoader(voc2007_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float).view(3, 1, 1)\n",
    "idx2class = {idx: label_name for label_name, idx in classes2idx.items()}\n",
    "\n",
    "for i, voc_data in enumerate(iter(voc_loader)):\n",
    "    samples, targets, sample_infos = voc_data\n",
    "    for sample, target in zip(samples, targets):\n",
    "        image = ((sample * std + mean) * 255).permute(1, 2, 0).to(torch.uint8).cpu().numpy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        boxes = target['boxes'].data.cpu().numpy().astype(np.int32)\n",
    "        labels = target['labels'].data.cpu().numpy().astype(np.int32)\n",
    "        for box, label in zip(boxes, labels):\n",
    "            if label != -1:\n",
    "                image = np.ascontiguousarray(image)\n",
    "                cv2.rectangle(img=image, pt1=(int(box[0]), int(box[1])), pt2=(int(box[2]), int(box[3])),\n",
    "                              color=(0, 255, 0), thickness=1)\n",
    "                cv2.putText(img=image, text=idx2class[int(label)], org=tuple(box[:2]),\n",
    "                            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.001 * max(image.shape[0], image.shape[1]),\n",
    "                            color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('image', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d269e36-c455-4323-9ec7-9558af287faf",
   "metadata": {},
   "source": [
    "# Dataset TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57db2db5-1ab5-4def-8861-21c4603f956e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 216, 'test': 65, 'valid': 54}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "\n",
    "datadir = Path('./dataset/TABLE/')\n",
    "image_pattern = '**/*.jpg'\n",
    "label_pattern = '**/*.json'\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "for set_name in ['train', 'test', 'valid']:\n",
    "    image_paths = natsorted(datadir.joinpath(set_name).glob(image_pattern), key=lambda x: x.stem)\n",
    "    label_paths = natsorted(datadir.joinpath(set_name).glob(label_pattern), key=lambda x: x.stem)\n",
    "    data_pairs = [(image_path, label_path) for image_path, label_path in zip(image_paths, label_paths) if image_path.stem == label_path.stem]\n",
    "    statistics[set_name] = len(data_pairs)\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1beb3ac-46e3-48bc-b32e-90f16ed91ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 216\n",
      "number of dataset: 216\n"
     ]
    }
   ],
   "source": [
    "from flame.core.data.table_dataset import TableDataset\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "classes = {'TABLE': 0, 'borderless_table': 0, 'bordered_table': 0, 'color_table': 0}\n",
    "\n",
    "transforms = [iaa.Add(value=(-10, 10), per_channel=True),\n",
    "              iaa.GaussianBlur(sigma=(0, 1)),\n",
    "              iaa.MotionBlur(),\n",
    "              iaa.JpegCompression(compression=(0, 10)),\n",
    "              iaa.Fliplr(p=0.5),\n",
    "              iaa.Flipud(p=0.5),\n",
    "              iaa.Grayscale(alpha=(0.0, 0.1)),\n",
    "#               iaa.Rot90(k=[0, 1, 2, 3], keep_size=False),\n",
    "              iaa.Affine(rotate=(-5, 5), shear=(-5, 5), fit_output=True),\n",
    "              iaa.Crop(percent=(0, 0.1)),\n",
    "              iaa.Pad(percent=(0, 0.1), keep_size=False),\n",
    "              iaa.ChangeColorTemperature()]\n",
    "\n",
    "\n",
    "table_dataset = TableDataset(dirname='./dataset/TABLE/train/', image_patterns=['*.jpg'], label_patterns=['*.json'],\n",
    "                             mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], compound_coef=0,\n",
    "                             classes=classes, transforms=transforms)\n",
    "\n",
    "print(f'number of dataset: {len(table_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fbc583d-1278-46e4-ac09-abe16c4f32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_loader = DataLoader(table_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float).view(3, 1, 1)\n",
    "idx2class = {idx: label_name for label_name, idx in classes.items()}\n",
    "\n",
    "for i, voc_data in enumerate(iter(voc_loader)):\n",
    "    samples, targets, sample_infos = voc_data\n",
    "    for sample, target in zip(samples, targets):\n",
    "        image = ((sample * std + mean) * 255).permute(1, 2, 0).to(torch.uint8).cpu().numpy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        boxes = target['boxes'].data.cpu().numpy().astype(np.int32)\n",
    "        labels = target['labels'].data.cpu().numpy().astype(np.int32)\n",
    "        for box, label in zip(boxes, labels):\n",
    "            if label != -1:\n",
    "                image = np.ascontiguousarray(image)\n",
    "                cv2.rectangle(img=image, pt1=(int(box[0]), int(box[1])), pt2=(int(box[2]), int(box[3])),\n",
    "                              color=(0, 255, 0), thickness=1)\n",
    "                cv2.putText(img=image, text=idx2class[int(label)], org=tuple(box[:2]),\n",
    "                            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.001 * max(image.shape[0], image.shape[1]),\n",
    "                            color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('image', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cdf72-6115-4a4b-b5cf-a09ada3581cb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3808523-ae0c-49cd-8c24-743791adedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flame.core.model.efficientdet import EfficientDet\n",
    "\n",
    "pretrained_weight = './checkpoint/efficientdet_pretrained_weight/efficientdet-d1.pth'\n",
    "head_only = False\n",
    "num_classes = 20\n",
    "compound_coef = 1\n",
    "backbone_pretrained = False\n",
    "scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n",
    "aspect_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n",
    "iou_threshold = 0.2\n",
    "score_threshold = 0.2\n",
    "\n",
    "model = EfficientDet(pretrained_weight=pretrained_weight,\n",
    "                     head_only=head_only,\n",
    "                     num_classes=num_classes,\n",
    "                     compound_coef=compound_coef,\n",
    "                     backbone_pretrained=backbone_pretrained,\n",
    "                     scales=scales, aspect_ratios=aspect_ratios,\n",
    "                     iou_threshold=iou_threshold,\n",
    "                     score_threshold=score_threshold)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2418cbb-f125-43c1-8e9f-f5bc99a08a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6569828\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509dba0f-f9b3-4e8f-93c0-782bc95cba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+------------+\n",
      "|                             Modules                             | Parameters |\n",
      "+-----------------------------------------------------------------+------------+\n",
      "|                       model.bifpn.0.p6_w1                       |     2      |\n",
      "|                       model.bifpn.0.p5_w1                       |     2      |\n",
      "|                       model.bifpn.0.p4_w1                       |     2      |\n",
      "|                       model.bifpn.0.p3_w1                       |     2      |\n",
      "|                       model.bifpn.0.p4_w2                       |     3      |\n",
      "|                       model.bifpn.0.p5_w2                       |     3      |\n",
      "|                       model.bifpn.0.p6_w2                       |     3      |\n",
      "|                       model.bifpn.0.p7_w2                       |     2      |\n",
      "|        model.bifpn.0.conv6_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv6_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv6_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv6_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv6_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.0.conv5_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv5_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv5_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv5_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv5_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.0.conv4_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv4_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv4_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv4_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv4_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.0.conv3_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.0.conv3_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.0.conv3_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.0.conv3_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.0.conv3_up.bn.bias                 |     64     |\n",
      "|       model.bifpn.0.conv4_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv4_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv4_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv4_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv4_down.bn.bias                |     64     |\n",
      "|       model.bifpn.0.conv5_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv5_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv5_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv5_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv5_down.bn.bias                |     64     |\n",
      "|       model.bifpn.0.conv6_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv6_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv6_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv6_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv6_down.bn.bias                |     64     |\n",
      "|       model.bifpn.0.conv7_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.0.conv7_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.0.conv7_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.0.conv7_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.0.conv7_down.bn.bias                |     64     |\n",
      "|           model.bifpn.0.p5_down_channel.0.conv.weight           |   20480    |\n",
      "|            model.bifpn.0.p5_down_channel.0.conv.bias            |     64     |\n",
      "|              model.bifpn.0.p5_down_channel.1.weight             |     64     |\n",
      "|               model.bifpn.0.p5_down_channel.1.bias              |     64     |\n",
      "|           model.bifpn.0.p4_down_channel.0.conv.weight           |    7168    |\n",
      "|            model.bifpn.0.p4_down_channel.0.conv.bias            |     64     |\n",
      "|              model.bifpn.0.p4_down_channel.1.weight             |     64     |\n",
      "|               model.bifpn.0.p4_down_channel.1.bias              |     64     |\n",
      "|           model.bifpn.0.p3_down_channel.0.conv.weight           |    2560    |\n",
      "|            model.bifpn.0.p3_down_channel.0.conv.bias            |     64     |\n",
      "|              model.bifpn.0.p3_down_channel.1.weight             |     64     |\n",
      "|               model.bifpn.0.p3_down_channel.1.bias              |     64     |\n",
      "|               model.bifpn.0.p5_to_p6.0.conv.weight              |   20480    |\n",
      "|                model.bifpn.0.p5_to_p6.0.conv.bias               |     64     |\n",
      "|                 model.bifpn.0.p5_to_p6.1.weight                 |     64     |\n",
      "|                  model.bifpn.0.p5_to_p6.1.bias                  |     64     |\n",
      "|          model.bifpn.0.p4_down_channel_2.0.conv.weight          |    7168    |\n",
      "|           model.bifpn.0.p4_down_channel_2.0.conv.bias           |     64     |\n",
      "|             model.bifpn.0.p4_down_channel_2.1.weight            |     64     |\n",
      "|              model.bifpn.0.p4_down_channel_2.1.bias             |     64     |\n",
      "|          model.bifpn.0.p5_down_channel_2.0.conv.weight          |   20480    |\n",
      "|           model.bifpn.0.p5_down_channel_2.0.conv.bias           |     64     |\n",
      "|             model.bifpn.0.p5_down_channel_2.1.weight            |     64     |\n",
      "|              model.bifpn.0.p5_down_channel_2.1.bias             |     64     |\n",
      "|                       model.bifpn.1.p6_w1                       |     2      |\n",
      "|                       model.bifpn.1.p5_w1                       |     2      |\n",
      "|                       model.bifpn.1.p4_w1                       |     2      |\n",
      "|                       model.bifpn.1.p3_w1                       |     2      |\n",
      "|                       model.bifpn.1.p4_w2                       |     3      |\n",
      "|                       model.bifpn.1.p5_w2                       |     3      |\n",
      "|                       model.bifpn.1.p6_w2                       |     3      |\n",
      "|                       model.bifpn.1.p7_w2                       |     2      |\n",
      "|        model.bifpn.1.conv6_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv6_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv6_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv6_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv6_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.1.conv5_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv5_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv5_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv5_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv5_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.1.conv4_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv4_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv4_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv4_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv4_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.1.conv3_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.1.conv3_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.1.conv3_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.1.conv3_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.1.conv3_up.bn.bias                 |     64     |\n",
      "|       model.bifpn.1.conv4_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv4_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv4_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv4_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv4_down.bn.bias                |     64     |\n",
      "|       model.bifpn.1.conv5_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv5_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv5_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv5_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv5_down.bn.bias                |     64     |\n",
      "|       model.bifpn.1.conv6_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv6_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv6_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv6_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv6_down.bn.bias                |     64     |\n",
      "|       model.bifpn.1.conv7_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.1.conv7_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.1.conv7_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.1.conv7_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.1.conv7_down.bn.bias                |     64     |\n",
      "|                       model.bifpn.2.p6_w1                       |     2      |\n",
      "|                       model.bifpn.2.p5_w1                       |     2      |\n",
      "|                       model.bifpn.2.p4_w1                       |     2      |\n",
      "|                       model.bifpn.2.p3_w1                       |     2      |\n",
      "|                       model.bifpn.2.p4_w2                       |     3      |\n",
      "|                       model.bifpn.2.p5_w2                       |     3      |\n",
      "|                       model.bifpn.2.p6_w2                       |     3      |\n",
      "|                       model.bifpn.2.p7_w2                       |     2      |\n",
      "|        model.bifpn.2.conv6_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv6_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv6_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv6_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv6_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.2.conv5_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv5_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv5_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv5_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv5_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.2.conv4_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv4_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv4_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv4_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv4_up.bn.bias                 |     64     |\n",
      "|        model.bifpn.2.conv3_up.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.bifpn.2.conv3_up.pointwise_conv.conv.weight        |    4096    |\n",
      "|         model.bifpn.2.conv3_up.pointwise_conv.conv.bias         |     64     |\n",
      "|                 model.bifpn.2.conv3_up.bn.weight                |     64     |\n",
      "|                  model.bifpn.2.conv3_up.bn.bias                 |     64     |\n",
      "|       model.bifpn.2.conv4_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv4_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv4_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv4_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv4_down.bn.bias                |     64     |\n",
      "|       model.bifpn.2.conv5_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv5_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv5_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv5_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv5_down.bn.bias                |     64     |\n",
      "|       model.bifpn.2.conv6_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv6_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv6_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv6_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv6_down.bn.bias                |     64     |\n",
      "|       model.bifpn.2.conv7_down.depthwise_conv.conv.weight       |    576     |\n",
      "|       model.bifpn.2.conv7_down.pointwise_conv.conv.weight       |    4096    |\n",
      "|        model.bifpn.2.conv7_down.pointwise_conv.conv.bias        |     64     |\n",
      "|                model.bifpn.2.conv7_down.bn.weight               |     64     |\n",
      "|                 model.bifpn.2.conv7_down.bn.bias                |     64     |\n",
      "|      model.regressor.conv_list.0.depthwise_conv.conv.weight     |    576     |\n",
      "|      model.regressor.conv_list.0.pointwise_conv.conv.weight     |    4096    |\n",
      "|       model.regressor.conv_list.0.pointwise_conv.conv.bias      |     64     |\n",
      "|      model.regressor.conv_list.1.depthwise_conv.conv.weight     |    576     |\n",
      "|      model.regressor.conv_list.1.pointwise_conv.conv.weight     |    4096    |\n",
      "|       model.regressor.conv_list.1.pointwise_conv.conv.bias      |     64     |\n",
      "|      model.regressor.conv_list.2.depthwise_conv.conv.weight     |    576     |\n",
      "|      model.regressor.conv_list.2.pointwise_conv.conv.weight     |    4096    |\n",
      "|       model.regressor.conv_list.2.pointwise_conv.conv.bias      |     64     |\n",
      "|                model.regressor.bn_list.0.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.0.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.0.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.0.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.0.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.0.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.1.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.1.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.1.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.1.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.1.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.1.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.2.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.2.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.2.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.2.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.2.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.2.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.3.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.3.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.3.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.3.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.3.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.3.2.bias                |     64     |\n",
      "|                model.regressor.bn_list.4.0.weight               |     64     |\n",
      "|                 model.regressor.bn_list.4.0.bias                |     64     |\n",
      "|                model.regressor.bn_list.4.1.weight               |     64     |\n",
      "|                 model.regressor.bn_list.4.1.bias                |     64     |\n",
      "|                model.regressor.bn_list.4.2.weight               |     64     |\n",
      "|                 model.regressor.bn_list.4.2.bias                |     64     |\n",
      "|        model.regressor.header.depthwise_conv.conv.weight        |    576     |\n",
      "|        model.regressor.header.pointwise_conv.conv.weight        |    2304    |\n",
      "|         model.regressor.header.pointwise_conv.conv.bias         |     36     |\n",
      "|     model.classifier.conv_list.0.depthwise_conv.conv.weight     |    576     |\n",
      "|     model.classifier.conv_list.0.pointwise_conv.conv.weight     |    4096    |\n",
      "|      model.classifier.conv_list.0.pointwise_conv.conv.bias      |     64     |\n",
      "|     model.classifier.conv_list.1.depthwise_conv.conv.weight     |    576     |\n",
      "|     model.classifier.conv_list.1.pointwise_conv.conv.weight     |    4096    |\n",
      "|      model.classifier.conv_list.1.pointwise_conv.conv.bias      |     64     |\n",
      "|     model.classifier.conv_list.2.depthwise_conv.conv.weight     |    576     |\n",
      "|     model.classifier.conv_list.2.pointwise_conv.conv.weight     |    4096    |\n",
      "|      model.classifier.conv_list.2.pointwise_conv.conv.bias      |     64     |\n",
      "|               model.classifier.bn_list.0.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.0.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.0.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.0.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.0.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.0.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.1.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.1.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.1.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.1.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.1.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.1.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.2.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.2.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.2.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.2.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.2.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.2.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.3.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.3.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.3.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.3.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.3.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.3.2.bias                |     64     |\n",
      "|               model.classifier.bn_list.4.0.weight               |     64     |\n",
      "|                model.classifier.bn_list.4.0.bias                |     64     |\n",
      "|               model.classifier.bn_list.4.1.weight               |     64     |\n",
      "|                model.classifier.bn_list.4.1.bias                |     64     |\n",
      "|               model.classifier.bn_list.4.2.weight               |     64     |\n",
      "|                model.classifier.bn_list.4.2.bias                |     64     |\n",
      "|        model.classifier.header.depthwise_conv.conv.weight       |    576     |\n",
      "|        model.classifier.header.pointwise_conv.conv.weight       |   11520    |\n",
      "|         model.classifier.header.pointwise_conv.conv.bias        |    180     |\n",
      "|         model.backbone_net.model._conv_stem.conv.weight         |    864     |\n",
      "|               model.backbone_net.model._bn0.weight              |     32     |\n",
      "|                model.backbone_net.model._bn0.bias               |     32     |\n",
      "|  model.backbone_net.model._blocks.0._depthwise_conv.conv.weight |    288     |\n",
      "|          model.backbone_net.model._blocks.0._bn1.weight         |     32     |\n",
      "|           model.backbone_net.model._blocks.0._bn1.bias          |     32     |\n",
      "|    model.backbone_net.model._blocks.0._se_reduce.conv.weight    |    256     |\n",
      "|     model.backbone_net.model._blocks.0._se_reduce.conv.bias     |     8      |\n",
      "|    model.backbone_net.model._blocks.0._se_expand.conv.weight    |    256     |\n",
      "|     model.backbone_net.model._blocks.0._se_expand.conv.bias     |     32     |\n",
      "|   model.backbone_net.model._blocks.0._project_conv.conv.weight  |    512     |\n",
      "|          model.backbone_net.model._blocks.0._bn2.weight         |     16     |\n",
      "|           model.backbone_net.model._blocks.0._bn2.bias          |     16     |\n",
      "|   model.backbone_net.model._blocks.1._expand_conv.conv.weight   |    1536    |\n",
      "|          model.backbone_net.model._blocks.1._bn0.weight         |     96     |\n",
      "|           model.backbone_net.model._blocks.1._bn0.bias          |     96     |\n",
      "|  model.backbone_net.model._blocks.1._depthwise_conv.conv.weight |    864     |\n",
      "|          model.backbone_net.model._blocks.1._bn1.weight         |     96     |\n",
      "|           model.backbone_net.model._blocks.1._bn1.bias          |     96     |\n",
      "|    model.backbone_net.model._blocks.1._se_reduce.conv.weight    |    384     |\n",
      "|     model.backbone_net.model._blocks.1._se_reduce.conv.bias     |     4      |\n",
      "|    model.backbone_net.model._blocks.1._se_expand.conv.weight    |    384     |\n",
      "|     model.backbone_net.model._blocks.1._se_expand.conv.bias     |     96     |\n",
      "|   model.backbone_net.model._blocks.1._project_conv.conv.weight  |    2304    |\n",
      "|          model.backbone_net.model._blocks.1._bn2.weight         |     24     |\n",
      "|           model.backbone_net.model._blocks.1._bn2.bias          |     24     |\n",
      "|   model.backbone_net.model._blocks.2._expand_conv.conv.weight   |    3456    |\n",
      "|          model.backbone_net.model._blocks.2._bn0.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.2._bn0.bias          |    144     |\n",
      "|  model.backbone_net.model._blocks.2._depthwise_conv.conv.weight |    1296    |\n",
      "|          model.backbone_net.model._blocks.2._bn1.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.2._bn1.bias          |    144     |\n",
      "|    model.backbone_net.model._blocks.2._se_reduce.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.2._se_reduce.conv.bias     |     6      |\n",
      "|    model.backbone_net.model._blocks.2._se_expand.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.2._se_expand.conv.bias     |    144     |\n",
      "|   model.backbone_net.model._blocks.2._project_conv.conv.weight  |    3456    |\n",
      "|          model.backbone_net.model._blocks.2._bn2.weight         |     24     |\n",
      "|           model.backbone_net.model._blocks.2._bn2.bias          |     24     |\n",
      "|   model.backbone_net.model._blocks.3._expand_conv.conv.weight   |    3456    |\n",
      "|          model.backbone_net.model._blocks.3._bn0.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.3._bn0.bias          |    144     |\n",
      "|  model.backbone_net.model._blocks.3._depthwise_conv.conv.weight |    3600    |\n",
      "|          model.backbone_net.model._blocks.3._bn1.weight         |    144     |\n",
      "|           model.backbone_net.model._blocks.3._bn1.bias          |    144     |\n",
      "|    model.backbone_net.model._blocks.3._se_reduce.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.3._se_reduce.conv.bias     |     6      |\n",
      "|    model.backbone_net.model._blocks.3._se_expand.conv.weight    |    864     |\n",
      "|     model.backbone_net.model._blocks.3._se_expand.conv.bias     |    144     |\n",
      "|   model.backbone_net.model._blocks.3._project_conv.conv.weight  |    5760    |\n",
      "|          model.backbone_net.model._blocks.3._bn2.weight         |     40     |\n",
      "|           model.backbone_net.model._blocks.3._bn2.bias          |     40     |\n",
      "|   model.backbone_net.model._blocks.4._expand_conv.conv.weight   |    9600    |\n",
      "|          model.backbone_net.model._blocks.4._bn0.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.4._bn0.bias          |    240     |\n",
      "|  model.backbone_net.model._blocks.4._depthwise_conv.conv.weight |    6000    |\n",
      "|          model.backbone_net.model._blocks.4._bn1.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.4._bn1.bias          |    240     |\n",
      "|    model.backbone_net.model._blocks.4._se_reduce.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.4._se_reduce.conv.bias     |     10     |\n",
      "|    model.backbone_net.model._blocks.4._se_expand.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.4._se_expand.conv.bias     |    240     |\n",
      "|   model.backbone_net.model._blocks.4._project_conv.conv.weight  |    9600    |\n",
      "|          model.backbone_net.model._blocks.4._bn2.weight         |     40     |\n",
      "|           model.backbone_net.model._blocks.4._bn2.bias          |     40     |\n",
      "|   model.backbone_net.model._blocks.5._expand_conv.conv.weight   |    9600    |\n",
      "|          model.backbone_net.model._blocks.5._bn0.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.5._bn0.bias          |    240     |\n",
      "|  model.backbone_net.model._blocks.5._depthwise_conv.conv.weight |    2160    |\n",
      "|          model.backbone_net.model._blocks.5._bn1.weight         |    240     |\n",
      "|           model.backbone_net.model._blocks.5._bn1.bias          |    240     |\n",
      "|    model.backbone_net.model._blocks.5._se_reduce.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.5._se_reduce.conv.bias     |     10     |\n",
      "|    model.backbone_net.model._blocks.5._se_expand.conv.weight    |    2400    |\n",
      "|     model.backbone_net.model._blocks.5._se_expand.conv.bias     |    240     |\n",
      "|   model.backbone_net.model._blocks.5._project_conv.conv.weight  |   19200    |\n",
      "|          model.backbone_net.model._blocks.5._bn2.weight         |     80     |\n",
      "|           model.backbone_net.model._blocks.5._bn2.bias          |     80     |\n",
      "|   model.backbone_net.model._blocks.6._expand_conv.conv.weight   |   38400    |\n",
      "|          model.backbone_net.model._blocks.6._bn0.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.6._bn0.bias          |    480     |\n",
      "|  model.backbone_net.model._blocks.6._depthwise_conv.conv.weight |    4320    |\n",
      "|          model.backbone_net.model._blocks.6._bn1.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.6._bn1.bias          |    480     |\n",
      "|    model.backbone_net.model._blocks.6._se_reduce.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.6._se_reduce.conv.bias     |     20     |\n",
      "|    model.backbone_net.model._blocks.6._se_expand.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.6._se_expand.conv.bias     |    480     |\n",
      "|   model.backbone_net.model._blocks.6._project_conv.conv.weight  |   38400    |\n",
      "|          model.backbone_net.model._blocks.6._bn2.weight         |     80     |\n",
      "|           model.backbone_net.model._blocks.6._bn2.bias          |     80     |\n",
      "|   model.backbone_net.model._blocks.7._expand_conv.conv.weight   |   38400    |\n",
      "|          model.backbone_net.model._blocks.7._bn0.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.7._bn0.bias          |    480     |\n",
      "|  model.backbone_net.model._blocks.7._depthwise_conv.conv.weight |    4320    |\n",
      "|          model.backbone_net.model._blocks.7._bn1.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.7._bn1.bias          |    480     |\n",
      "|    model.backbone_net.model._blocks.7._se_reduce.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.7._se_reduce.conv.bias     |     20     |\n",
      "|    model.backbone_net.model._blocks.7._se_expand.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.7._se_expand.conv.bias     |    480     |\n",
      "|   model.backbone_net.model._blocks.7._project_conv.conv.weight  |   38400    |\n",
      "|          model.backbone_net.model._blocks.7._bn2.weight         |     80     |\n",
      "|           model.backbone_net.model._blocks.7._bn2.bias          |     80     |\n",
      "|   model.backbone_net.model._blocks.8._expand_conv.conv.weight   |   38400    |\n",
      "|          model.backbone_net.model._blocks.8._bn0.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.8._bn0.bias          |    480     |\n",
      "|  model.backbone_net.model._blocks.8._depthwise_conv.conv.weight |   12000    |\n",
      "|          model.backbone_net.model._blocks.8._bn1.weight         |    480     |\n",
      "|           model.backbone_net.model._blocks.8._bn1.bias          |    480     |\n",
      "|    model.backbone_net.model._blocks.8._se_reduce.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.8._se_reduce.conv.bias     |     20     |\n",
      "|    model.backbone_net.model._blocks.8._se_expand.conv.weight    |    9600    |\n",
      "|     model.backbone_net.model._blocks.8._se_expand.conv.bias     |    480     |\n",
      "|   model.backbone_net.model._blocks.8._project_conv.conv.weight  |   53760    |\n",
      "|          model.backbone_net.model._blocks.8._bn2.weight         |    112     |\n",
      "|           model.backbone_net.model._blocks.8._bn2.bias          |    112     |\n",
      "|   model.backbone_net.model._blocks.9._expand_conv.conv.weight   |   75264    |\n",
      "|          model.backbone_net.model._blocks.9._bn0.weight         |    672     |\n",
      "|           model.backbone_net.model._blocks.9._bn0.bias          |    672     |\n",
      "|  model.backbone_net.model._blocks.9._depthwise_conv.conv.weight |   16800    |\n",
      "|          model.backbone_net.model._blocks.9._bn1.weight         |    672     |\n",
      "|           model.backbone_net.model._blocks.9._bn1.bias          |    672     |\n",
      "|    model.backbone_net.model._blocks.9._se_reduce.conv.weight    |   18816    |\n",
      "|     model.backbone_net.model._blocks.9._se_reduce.conv.bias     |     28     |\n",
      "|    model.backbone_net.model._blocks.9._se_expand.conv.weight    |   18816    |\n",
      "|     model.backbone_net.model._blocks.9._se_expand.conv.bias     |    672     |\n",
      "|   model.backbone_net.model._blocks.9._project_conv.conv.weight  |   75264    |\n",
      "|          model.backbone_net.model._blocks.9._bn2.weight         |    112     |\n",
      "|           model.backbone_net.model._blocks.9._bn2.bias          |    112     |\n",
      "|   model.backbone_net.model._blocks.10._expand_conv.conv.weight  |   75264    |\n",
      "|         model.backbone_net.model._blocks.10._bn0.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.10._bn0.bias          |    672     |\n",
      "| model.backbone_net.model._blocks.10._depthwise_conv.conv.weight |   16800    |\n",
      "|         model.backbone_net.model._blocks.10._bn1.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.10._bn1.bias          |    672     |\n",
      "|    model.backbone_net.model._blocks.10._se_reduce.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.10._se_reduce.conv.bias    |     28     |\n",
      "|    model.backbone_net.model._blocks.10._se_expand.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.10._se_expand.conv.bias    |    672     |\n",
      "|  model.backbone_net.model._blocks.10._project_conv.conv.weight  |   75264    |\n",
      "|         model.backbone_net.model._blocks.10._bn2.weight         |    112     |\n",
      "|          model.backbone_net.model._blocks.10._bn2.bias          |    112     |\n",
      "|   model.backbone_net.model._blocks.11._expand_conv.conv.weight  |   75264    |\n",
      "|         model.backbone_net.model._blocks.11._bn0.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.11._bn0.bias          |    672     |\n",
      "| model.backbone_net.model._blocks.11._depthwise_conv.conv.weight |   16800    |\n",
      "|         model.backbone_net.model._blocks.11._bn1.weight         |    672     |\n",
      "|          model.backbone_net.model._blocks.11._bn1.bias          |    672     |\n",
      "|    model.backbone_net.model._blocks.11._se_reduce.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.11._se_reduce.conv.bias    |     28     |\n",
      "|    model.backbone_net.model._blocks.11._se_expand.conv.weight   |   18816    |\n",
      "|     model.backbone_net.model._blocks.11._se_expand.conv.bias    |    672     |\n",
      "|  model.backbone_net.model._blocks.11._project_conv.conv.weight  |   129024   |\n",
      "|         model.backbone_net.model._blocks.11._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.11._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.12._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.12._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.12._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.12._depthwise_conv.conv.weight |   28800    |\n",
      "|         model.backbone_net.model._blocks.12._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.12._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.12._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.12._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.12._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.12._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.12._project_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.12._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.12._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.13._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.13._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.13._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.13._depthwise_conv.conv.weight |   28800    |\n",
      "|         model.backbone_net.model._blocks.13._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.13._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.13._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.13._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.13._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.13._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.13._project_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.13._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.13._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.14._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.14._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.14._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.14._depthwise_conv.conv.weight |   28800    |\n",
      "|         model.backbone_net.model._blocks.14._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.14._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.14._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.14._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.14._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.14._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.14._project_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.14._bn2.weight         |    192     |\n",
      "|          model.backbone_net.model._blocks.14._bn2.bias          |    192     |\n",
      "|   model.backbone_net.model._blocks.15._expand_conv.conv.weight  |   221184   |\n",
      "|         model.backbone_net.model._blocks.15._bn0.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.15._bn0.bias          |    1152    |\n",
      "| model.backbone_net.model._blocks.15._depthwise_conv.conv.weight |   10368    |\n",
      "|         model.backbone_net.model._blocks.15._bn1.weight         |    1152    |\n",
      "|          model.backbone_net.model._blocks.15._bn1.bias          |    1152    |\n",
      "|    model.backbone_net.model._blocks.15._se_reduce.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.15._se_reduce.conv.bias    |     48     |\n",
      "|    model.backbone_net.model._blocks.15._se_expand.conv.weight   |   55296    |\n",
      "|     model.backbone_net.model._blocks.15._se_expand.conv.bias    |    1152    |\n",
      "|  model.backbone_net.model._blocks.15._project_conv.conv.weight  |   368640   |\n",
      "|         model.backbone_net.model._blocks.15._bn2.weight         |    320     |\n",
      "|          model.backbone_net.model._blocks.15._bn2.bias          |    320     |\n",
      "+-----------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 3839117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3839117"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "290f88d6-7be2-4a19-9b98-ffbb38c84b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_iter = iter(voc_loader)\n",
    "samples, targets, image_infos = voc_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d98da99-b16a-42c0-861e-cb6d8cc0bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 49104, 20])\n",
      "torch.Size([2, 49104, 4])\n",
      "torch.Size([1, 49104, 4])\n"
     ]
    }
   ],
   "source": [
    "cls_preds, reg_preds, anchors = model(samples)\n",
    "print(cls_preds.shape)\n",
    "print(reg_preds.shape)\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eac498-ca7d-42c3-a4c0-33db8c5ba7d6",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea32470-a543-4338-abcf-1391588759fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.loss.focal_loss import FocalLoss\n",
    "\n",
    "loss = FocalLoss(alpha=0.25, gamma=2.0, lamda=50.0, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "326e7773-c3bb-4a29-a190-dbce95f7ee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10546.2178], grad_fn=<MeanBackward1>),\n",
       " tensor([3.1572], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(cls_preds, reg_preds, anchors, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb566519-028b-4014-a39a-12da3fbe8fa8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4ef7e1-3b67-4db0-a4ed-0a961e0ff45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.model.efficientdet import EfficientDet\n",
    "\n",
    "compound_coef = 0\n",
    "\n",
    "model = EfficientDet(pretrained_weight=None,\n",
    "                     head_only=False,\n",
    "                     num_classes=90,\n",
    "                     compound_coef=compound_coef,\n",
    "                     backbone_pretrained=False,\n",
    "                     score_threshold=0.3)\n",
    "model.load_state_dict(torch.load(f'./checkpoint/efficientdet_pretrained_weight/efficientdet-d{compound_coef}.pth', map_location='cpu'))\n",
    "# model.requires_grad_(False)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d7ce2a-ff7a-4eff-909a-dd77c306ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "            'fire hydrant', '', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "            'cow', 'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '', 'handbag', 'tie',\n",
    "            'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "            'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "            'cake', 'chair', 'couch', 'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n",
    "            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "            'refrigerator', '', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950e6462-052b-42ad-a692-93383c00993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "pad_to_square = iaa.PadToSquare(position='right-bottom')\n",
    "device = 'cpu'\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float).view(1, 3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float).view(1, 3, 1, 1)\n",
    "\n",
    "imsize = 512 + 128 * compound_coef\n",
    "\n",
    "def preprocess(image_paths, imsize=imsize, mean=mean, std=std, device=device):\n",
    "    images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "    padded_images = [pad_to_square(image=image) for image in images]\n",
    "    samples = [cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB) for padded_image in padded_images]\n",
    "    samples = [cv2.resize(sample, dsize=(imsize, imsize)) for sample in samples]\n",
    "    samples = [torch.from_numpy(sample) for sample in samples]\n",
    "    samples = torch.stack(samples, dim=0).to(device)\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = (samples.float().div(255.) - mean) / std\n",
    "    scales = [max(*image.shape) / imsize for image in images]\n",
    "    return images, scales, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ead04d55-c724-4e1f-85d3-8e50c328177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = ['./dataset/VOC2007/valid/000002.jpg',\n",
    "#                './dataset/VOC2007/valid/000013.jpg',\n",
    "#                './dataset/VOC2007/valid/000030.jpg']\n",
    "image_paths = ['./dataset/VOC2007/valid/000002.jpg']\n",
    "images, scales, samples = preprocess(image_paths=image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f0f487-eb31-4ac9-8f71-4ede20ecd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction time: 0.3552844524383545s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    predictions = model(samples)\n",
    "t2 = time.time()\n",
    "print(f'prediction time: {t2 - t1}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26b3dcd3-2164-4d47-b63b-5c081199fa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[143.4082, 203.5088, 211.8467, 306.7638]]),\n",
       "  'labels': tensor([6]),\n",
       "  'scores': tensor([0.7441])}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab3a59d4-8dcc-49bb-83a1-4035ce16f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, scale, pred in zip(images, scales, predictions):\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    labels = pred['labels'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    class_names = [classes[label] for label in labels]\n",
    "    boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\n",
    "    boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\n",
    "    boxes = boxes.astype(np.int32)\n",
    "    for box, score, class_name in zip(boxes, scores, class_names):\n",
    "        color = (np.random.randint(200, 255),\n",
    "                 np.random.randint(50, 200),\n",
    "                 np.random.randint(0, 150))\n",
    "        cv2.putText(image,\n",
    "                    text=f'{class_name}',\n",
    "                    org=tuple(box[:2]),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.001 * max(image.shape[0], image.shape[1]),\n",
    "                    color=color,\n",
    "                    thickness=1,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "        cv2.rectangle(image, pt1=tuple(box[:2]), pt2=tuple(box[2:]), color=color, thickness=3)\n",
    "        cv2.imshow('a', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9ff9a-02e4-404d-9cf1-26ad47906aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43501336-661e-4d19-9378-b581aaa276c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SiLU(nn.Module):\n",
    "    '''\n",
    "    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n",
    "        SiLU(x) = x * sigmoid(x)\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1e4288-3380-493e-976b-ef21a52d024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(start=-10, end=10, step=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "252c56b0-6218-44c5-b0ed-09e23a9130d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_silu = SiLU()(x).numpy()\n",
    "y_relu = nn.ReLU6()(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f14edbfb-eb19-400c-ad2d-204a632066b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ee7ab29-d385-47ac-9872-1171863f0e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff7fdc5f050>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y_silu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "347d8678-2349-48c3-8f7b-4c8e21942508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff7fd9c8f50>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c93eb792-1e4a-463d-a733-71b1c84cf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.SEnet = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=1),  # B x C x H x W -> B x C x 1 x 1\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=reduced_dim, kernel_size=1),  # B x C x 1 x 1 -> B x C/r x 1 x 1\n",
    "            SiLU(),  # in original using ReLU\n",
    "            nn.Conv2d(in_channels=reduced_dim, out_channels=in_channels, kernel_size=1),  # B x C/r x 1 x 1 -> B x C x 1 x 1\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.SEnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0d17be2-3b3d-40c0-9325-fa7b1610caf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n"
     ]
    }
   ],
   "source": [
    "senet = SqueezeExcitation(in_channels=32, reduced_dim=8)\n",
    "\n",
    "print(sum(param.numel() for param in senet.parameters() if param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c472a93-5861-4ab7-acc7-cded68aa2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size=1)  # C x H x W -> C x 1 x 1\n",
    "        self.linear1 = nn.Linear(in_features=in_channels, out_features=reduced_dim)  # C x 1 x 1 -> C/r x 1 x 1\n",
    "        self.silu = SiLU()  # in original using ReLU\n",
    "        self.linear2 = nn.Linear(in_features=reduced_dim, out_features=in_channels)  # C/r x 1 x 1 -> C x 1 x 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, _, _ = x.shape\n",
    "\n",
    "        y = self.pool(x).reshape(B, -1)  # B, C\n",
    "        y = self.linear1(y)  # B, C/r\n",
    "        y = self.silu(y)  # B, C/r\n",
    "        y = self.linear2(y)  # B, C\n",
    "        y = nn.Sigmoid()(y)  # B, C\n",
    "        y = y.reshape(B, C, 1, 1)  # B, C, 1, 1\n",
    "\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af898b1a-fcdb-4982-b84f-7703a6c1fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n"
     ]
    }
   ],
   "source": [
    "senet = SqueezeExcitation(in_channels=32, reduced_dim=8)\n",
    "\n",
    "print(sum(param.numel() for param in senet.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888cd25f-52dc-4c34-8d65-30c4cda146e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec89644-cb6f-4e31-9066-c5c8fbab7d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b03fac-3f76-4a10-8e15-8808d2fad7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.array([1, 2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df2e45c-036d-4791-ba81-63dd6c41d287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37dc530-7054-4375-bb5c-4ccf1a8962e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a988ab-c1b5-4fd8-a5e7-6c41de65fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles[1::2] = angles[0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc9a0ada-a2e5-408d-8076-af9c238d71bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 4, 4, 6, 6, 8, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b004b-ccb2-45c2-8148-1da94cbd2d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
